{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Example format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diagnosis_1</th>\n",
       "      <th>Diagnosis_2</th>\n",
       "      <th>Diagnosis_3</th>\n",
       "      <th>Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>free text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diagnosis_1  Diagnosis_2  Diagnosis_3     Report\n",
       "0            0            0            1  free text\n",
       "1            0            1            0  free text\n",
       "2            0            0            1  free text\n",
       "3            1            0            0  free text\n",
       "4            1            0            0  free text\n",
       "5            1            0            0  free text\n",
       "6            0            0            1  free text\n",
       "7            0            1            0  free text\n",
       "8            0            0            1  free text\n",
       "9            0            1            0  free text"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This dummy dataframe depicts the format of the input data\n",
    "### The labels are are provided in a one-hot encoded fashion and as such mutually exclusive.\n",
    "import pandas as pd\n",
    "\n",
    "d = {'Diagnosis_1': [0, 0, 0, 1, 1, 1, 0, 0, 0, 0], \n",
    "     'Diagnosis_2': [0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "     'Diagnosis_3': [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
    "     'Report': ['free text']*10}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glioma</th>\n",
       "      <th>Meningioma</th>\n",
       "      <th>Metastasis</th>\n",
       "      <th>Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|Accession Number:  BL15A20488                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|Accession Number:  BL15A34100                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|Accession Number:  BL15D23219                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|Accession Number:  BL15D38566                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|Accession Number:  BL15D43181                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|Accession Number:  BL15E22467                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|Accession Number:  BL15E47811                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>|Accession Number:  BL15F23188                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|Accession Number:  BL15F24113                ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>|Accession Number:  BL15F25117                ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Glioma  Meningioma  Metastasis  \\\n",
       "0       1           0           0   \n",
       "1       1           0           0   \n",
       "2       1           0           0   \n",
       "3       1           0           0   \n",
       "4       1           0           0   \n",
       "5       1           0           0   \n",
       "6       1           0           0   \n",
       "7       0           0           1   \n",
       "8       1           0           0   \n",
       "9       1           0           0   \n",
       "\n",
       "                                              Report  \n",
       "0  |Accession Number:  BL15A20488                ...  \n",
       "1  |Accession Number:  BL15A34100                ...  \n",
       "2  |Accession Number:  BL15D23219                ...  \n",
       "3  |Accession Number:  BL15D38566                ...  \n",
       "4  |Accession Number:  BL15D43181                ...  \n",
       "5  |Accession Number:  BL15E22467                ...  \n",
       "6  |Accession Number:  BL15E47811                ...  \n",
       "7  |Accession Number:  BL15F23188                ...  \n",
       "8  |Accession Number:  BL15F24113                ...  \n",
       "9  |Accession Number:  BL15F25117                ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set directories\n",
    "# source_dir = ### ENTER SOURCE DIRECTORY ###\n",
    "\n",
    "# Import Excel file\n",
    "df = pd.read_excel(os.path.join(source_dir,'df_one_hot_encoded.xlsx'))\n",
    "\n",
    "df[['Glioma', 'Meningioma', 'Metastasis', 'Report']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A. Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Remove uneccesary patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    \n",
    "    #remove all text before 'CLINICAL DATA'\n",
    "    text = re.sub(r'.*CLINICAL DATA', 'CLINICAL DATA', text)\n",
    "    \n",
    "    #remove disclaimer and signature'\n",
    "    text = text.split('These tests were developed and their performance characteristics determined bythe Molecular Diagnostics Laboratory')[0]\n",
    "    \n",
    "    # Remove newlines\n",
    "    text = text.replace(r'\\n', ' ')\n",
    "    \n",
    "    # Remove date\n",
    "    date_pattern = r'[0-9]{1,2}[-/][0-9]{1,2}[-/][0-9]{2,}'\n",
    "    text = re.sub(date_pattern, ' ', text.lower())\n",
    "    \n",
    "    # Remove whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove punctution, keep decimal points\n",
    "    text = re.sub(r'[\\W]+(?!\\d)', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['cleaned_report'] = df['Report'].apply(preprocessor)\n",
    "df['cleaned_report'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2C. Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "stop_words_modified = [w for w in stop_words_english if w not in ['no', 'not']]\n",
    "\n",
    "def remove_words(clean_report): \n",
    "    clean_report_words = clean_report.split()\n",
    "    stripped_report_words  = [word for word in clean_report_words if word.lower() not in stop_words_modified]\n",
    "    stripped_report_text = ' '.join(stripped_report_words)\n",
    "    return stripped_report_text\n",
    "\n",
    "df['stripped_report'] = df['cleaned_report'].apply(remove_words)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def porter_stemming(text):\n",
    "    stemmed_report_words = [porter.stem(word) for word in text.split()]\n",
    "    stemmed_report_text = ' '.join(stemmed_report_words)\n",
    "    return stemmed_report_text\n",
    "\n",
    "df['stemmed_report'] = df['stripped_report'].apply(porter_stemming)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2E. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(report): \n",
    "    report = report.split()\n",
    "    return report\n",
    "\n",
    "df['tokenized_report'] = df['stemmed_report'].apply(split_words)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2F.  Train validation test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "n_patients = len(df)\n",
    "\n",
    "df = df.sample(frac = 1)\n",
    "list_train_test = ['train']*3000+['validation']*2000+['test']*2000\n",
    "df['cohort'] =  random.sample(list_train_test, len(list_train_test))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2F. Save Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = ### ENTER TARGET DIRECTORY ###\n",
    "df.to_excel(os.path.join(target_dir, 'preprocessed_reports.xlsx')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2H. Load Preprocessed Reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "source_dir = ### ENTER SOURCE DIRECTORY ###\n",
    "df = pd.read_excel(os.path.join(source_dir, 'preprocessed_reports.xlsx')) \n",
    "print('Dimensions df are', df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Statistical / Classic Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Create an parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid_dict = {'diagnosis': ['Glioma', 'Meningioma', 'Metastasis'],\n",
    "                    'l1': [1, 2, 4, 8, 16],\n",
    "                   'max_features':[100, 250, 500, 1000, 1500, 2000],\n",
    "                   'ngram_range': [1, 2, 3],\n",
    "                   'sample_size': [25, 50, 75, 100, 150, 200, 250, 300, 400, 500, 600, 800, 1000, 1200, 1500, 1800, 2100, 2500, 3000] \n",
    "                  }\n",
    "\n",
    "param_grid_list = list(ParameterGrid(param_grid_dict))\n",
    "param_grid_list\n",
    "\n",
    "parameter_grid = []\n",
    "for dict in param_grid_list:\n",
    "    hyperparameters = list(dict.values())\n",
    "    parameter_grid.append(hyperparameters)\n",
    "\n",
    "print('Length parameter grid is', len(parameter_grid), 'hyperparameter settings')\n",
    "parameter_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D. Hyperparameter tuning regression-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running this code block on a multiple cores or a cloud service might be desirable\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import linear_model\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "def square(list):\n",
    "    return [i ** 2 for i in list]\n",
    "\n",
    "diagnosis_column = []\n",
    "l1_column = []\n",
    "max_features_column = []\n",
    "ngram_range_column = []\n",
    "sample_size_column = []\n",
    "auc_mean_column = []\n",
    "auc_std_column = []\n",
    "number_of_bootstraps = []\n",
    "\n",
    "start_gridsearch = 1\n",
    "\n",
    "for diagnosis, l1, max_features, ngram_range, sample_size in parameter_grid:\n",
    "    \n",
    "    print('##### Grid Search', start_gridsearch,'/', len(parameter_grid))\n",
    "    \n",
    "    word_vectorizer = TfidfVectorizer(ngram_range=(1,ngram_range),max_features=max_features)\n",
    "    word_vectorizer.fit(df['tokenized_report'][df['cohort'].isin(['train', 'validation'])])\n",
    "  \n",
    "    bootstrap_aucs = []\n",
    "    \n",
    "    start_bootstrap = 1\n",
    "    \n",
    "    for n in range(int(3000//sample_size)):\n",
    "        \n",
    "        df_frac = df[(df['cohort'] == 'train')].sample(n = sample_size, replace = True)\n",
    "    \n",
    "        X_train = word_vectorizer.transform(df_frac['tokenized_report']).toarray()\n",
    "        y_train = np.asarray(df_frac[diagnosis])\n",
    "        \n",
    "        X_val = word_vectorizer.transform(df['tokenized_report'][df['cohort'].isin(['validation'])]).toarray()\n",
    "        y_val = np.asarray(df[diagnosis][df['cohort'].isin(['validation'])])\n",
    "        \n",
    "        try:\n",
    "            model_lasso = LogisticRegression(penalty = 'l1', C=l1, max_iter=200)\n",
    "            model_lasso.fit(X_train, y_train)\n",
    "\n",
    "            bootstrap_auc = roc_auc_score(y_val, model_lasso.predict_proba(X_val)[:,1])\n",
    "            bootstrap_aucs.append(bootstrap_auc)\n",
    "        \n",
    "        except ValueError:\n",
    "               pass\n",
    "                                            \n",
    "        start_bootstrap = start_bootstrap + 1 \n",
    "                                          \n",
    "    n_bootstraps = start_bootstrap - 1\n",
    "    \n",
    "    print('Number of bootstraps is', n_bootstraps)                                     \n",
    "                                          \n",
    "    auc_mean = np.mean(bootstrap_aucs)\n",
    "    auc_std = np.std(bootstrap_aucs)\n",
    "\n",
    "    print('The mean bootstrapped AUC for grid search', start_gridsearch, 'bootstrap ', start_bootstrap,\n",
    "          round(auc_mean,2),'±', round(auc_std,3))\n",
    "    \n",
    "    diagnosis_column.append(diagnosis)\n",
    "    l1_column.append(l1)\n",
    "    max_features_column.append(max_features)\n",
    "    ngram_range_column.append(ngram_range)\n",
    "    sample_size_column.append(sample_size)\n",
    "    auc_mean_column.append(auc_mean)\n",
    "    auc_std_column.append(auc_std)\n",
    "    number_of_bootstraps.append(n_bootstraps)\n",
    "    \n",
    "    start_gridsearch = start_gridsearch + 1\n",
    "    \n",
    "df_performance = pd.DataFrame(list(zip(diagnosis_column, sample_size_column, l1_column, max_features_column, ngram_range_column,\n",
    "                                    auc_mean_column, auc_std_column, number_of_bootstraps)))\n",
    "\n",
    "df_performance.columns = ['diagnosis', 'sample_size', 'l1', 'max_features', 'ngram_range',  'auc_mean', \n",
    "                          'auc_std', 'number_of_bootstraps']\n",
    "\n",
    "target_dir = ### ENTER TARGET DIRECTORY ###\n",
    "df_performance.to_excel(os.path.join(target_dir, 'hyperparameter_tuning.xlsx'))\n",
    "\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D. Select optimal hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "source_dir =  ### ENTER SOURCE DIRECTORY ###\n",
    "df_hyperparameter_results = pd.read_excel(os.path.join(source_dir, 'iteration_1.xlsx'))\n",
    "\n",
    "df_hyperparameter_optimal_lasso = df_hyperparameter_results.loc[df_hyperparameter_results.reset_index().groupby(['diagnosis', 'sample_size'])['auc_mean'].idxmax()]\n",
    "df_hyperparameter_optimal_lasso = df_hyperparameter_optimal_lasso[['diagnosis', 'l1', 'max_features', 'ngram_range', 'sample_size']]\n",
    "optimal_paramater_grid_lasso = df_hyperparameter_optimal_lasso.values.tolist()\n",
    "optimal_paramater_grid_lasso\n",
    "\n",
    "df_hyperparameter_results_logistic = df_hyperparameter_results.loc[df_hyperparameter_results['l1'] == 1]\n",
    "df_hyperparameter_optimal_logistic = df_hyperparameter_results_logistic.loc[df_hyperparameter_results_logistic.groupby(['diagnosis', 'sample_size'])['auc_mean'].idxmax()]\n",
    "df_hyperparameter_optimal_logistic = df_hyperparameter_optimal_logistic[['diagnosis', 'l1', 'max_features', 'ngram_range', 'sample_size']]\n",
    "optimal_paramater_grid_logistic = df_hyperparameter_optimal_logistic.values.tolist()\n",
    "optimal_paramater_grid_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3E. Train final models and compute predicted probabilities and classses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this code block twice. Once for Lasso, once for logistic regression.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import linear_model\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def square(list):\n",
    "    return [i ** 2 for i in list]\n",
    "\n",
    "diagnosis_column = []\n",
    "l1_column = []\n",
    "max_features_column = []\n",
    "ngram_range_column = []\n",
    "sample_size_column = []\n",
    "auc_mean = []\n",
    "auc_std = []\n",
    "number_of_bootstraps = []\n",
    "\n",
    "gridsearch = 1\n",
    "\n",
    "for diagnosis, l1, max_features, ngram_range, sample_size in optimal_paramater_grid_lasso:\n",
    "    \n",
    "    print('##### Grid Search', gridsearch,'/', len(optimal_paramater_grid_lasso))\n",
    "    \n",
    "    word_vectorizer = TfidfVectorizer(ngram_range=(1,ngram_range),max_features=max_features)\n",
    "    word_vectorizer.fit(df['tokenized_report'][df['cohort'].isin(['train', 'test'])])\n",
    "\n",
    "    bootstrap_aucs = [] \n",
    "    \n",
    "    start_bootstrap = 1\n",
    "    \n",
    "    for n in range(100):\n",
    "        \n",
    "        df_frac = df[(df['cohort'] == 'train')].sample(n = sample_size, replace = True)\n",
    "    \n",
    "        X_train = word_vectorizer.transform(df_frac['tokenized_report']).toarray()\n",
    "        y_train = np.asarray(df_frac[diagnosis])\n",
    "        \n",
    "        X_test = word_vectorizer.transform(df['tokenized_report'][df['cohort'].isin(['test'])]).toarray()\n",
    "        y_test = np.asarray(df[diagnosis][df['cohort'].isin(['test'])])\n",
    "        \n",
    "        try:\n",
    "            model = LogisticRegression(penalty = 'l1', C=l1, max_iter=200)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            predicted_probabilities = model.predict_proba(X_test)[:,1]\n",
    "            auc = roc_auc_score(y_test, predicted_probabilities)\n",
    "            print('AUC', auc)\n",
    "\n",
    "        except ValueError:\n",
    "              pass\n",
    "        \n",
    "        print('AUC:', str(round(auc,3)))\n",
    "        bootstrap_aucs.append(auc)\n",
    "        \n",
    "        gridsearch += 1 \n",
    "                                          \n",
    "    n_bootstraps = gridsearch - 1\n",
    "    print('### Number of bootstraps is', n_bootstraps)\n",
    "    \n",
    "    bootstrap_auc_mean = np.mean(bootstrap_aucs)\n",
    "    bootstrap_auc_std = np.std(bootstrap_aucs)\n",
    "    print('### Mean AUC for grid search', gridsearch, ':', round(bootstrap_auc_mean,2),'±', round(bootstrap_auc_std,3))\n",
    "\n",
    "    print('')\n",
    "    \n",
    "    diagnosis_column.append(diagnosis)\n",
    "    l1_column.append(l1)\n",
    "    max_features_column.append(max_features)\n",
    "    ngram_range_column.append(ngram_range)\n",
    "    sample_size_column.append(sample_size)\n",
    "    auc_mean.append(bootstrap_auc_mean)\n",
    "    auc_std.append(bootstrap_auc_std)\n",
    "    number_of_bootstraps.append(n_bootstraps)\n",
    "    \n",
    "    start_gridsearch = start_gridsearch + 1\n",
    "    \n",
    "df_performance = pd.DataFrame(list(zip(diagnosis_column, sample_size_column, l1_column, max_features_column, ngram_range_column,\n",
    "                                    auc_mean, auc_std, number_of_bootstraps)))\n",
    "\n",
    "df_performance.columns = ['diagnosis', 'sample_size', 'l1', 'max_features', 'ngram_range',  'auc_mean', \n",
    "                          'auc_std', 'number_of_bootstraps']\n",
    "\n",
    "target_dir = ### ENTER TARGET DIRECTORY ###\n",
    "\n",
    "df_performance.to_excel(os.path.join(target_dir, 'final_results_lasso.xlsx'))\n",
    "\n",
    "df_performance\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3F. Get results lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>pooled_auc</th>\n",
       "      <th>pooled_auc_std</th>\n",
       "      <th>auc_mean_glioma</th>\n",
       "      <th>auc_std_glioma</th>\n",
       "      <th>auc_mean_meningioma</th>\n",
       "      <th>auc_std_glioma</th>\n",
       "      <th>auc_mean_metastasis</th>\n",
       "      <th>auc_std_metastasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.712994</td>\n",
       "      <td>0.098449</td>\n",
       "      <td>0.645773</td>\n",
       "      <td>0.072372</td>\n",
       "      <td>0.800798</td>\n",
       "      <td>0.072372</td>\n",
       "      <td>0.692411</td>\n",
       "      <td>0.110357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.835223</td>\n",
       "      <td>0.068722</td>\n",
       "      <td>0.773859</td>\n",
       "      <td>0.072240</td>\n",
       "      <td>0.891660</td>\n",
       "      <td>0.072240</td>\n",
       "      <td>0.840152</td>\n",
       "      <td>0.081601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>0.899991</td>\n",
       "      <td>0.034488</td>\n",
       "      <td>0.897357</td>\n",
       "      <td>0.032403</td>\n",
       "      <td>0.913687</td>\n",
       "      <td>0.032403</td>\n",
       "      <td>0.888931</td>\n",
       "      <td>0.039001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.928525</td>\n",
       "      <td>0.018110</td>\n",
       "      <td>0.934140</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>0.927562</td>\n",
       "      <td>0.016937</td>\n",
       "      <td>0.923872</td>\n",
       "      <td>0.019718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.948871</td>\n",
       "      <td>0.012130</td>\n",
       "      <td>0.954875</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.942720</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.949018</td>\n",
       "      <td>0.011717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.958270</td>\n",
       "      <td>0.010108</td>\n",
       "      <td>0.966532</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>0.948886</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>0.959393</td>\n",
       "      <td>0.011541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>0.964613</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>0.970602</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.957131</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.966105</td>\n",
       "      <td>0.007668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>0.967522</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.969216</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>0.962033</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>0.971317</td>\n",
       "      <td>0.005340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400</td>\n",
       "      <td>0.973507</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>0.977039</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.968108</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.975375</td>\n",
       "      <td>0.005917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>0.975880</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.977130</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.971987</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.978522</td>\n",
       "      <td>0.003370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>600</td>\n",
       "      <td>0.977754</td>\n",
       "      <td>0.004787</td>\n",
       "      <td>0.981096</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.970941</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.981225</td>\n",
       "      <td>0.003265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>800</td>\n",
       "      <td>0.980467</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.982575</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.977045</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.981782</td>\n",
       "      <td>0.002548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.982947</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.983436</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.979853</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.985551</td>\n",
       "      <td>0.001871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.983998</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.984504</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.981602</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.985886</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.984945</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.984715</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.983677</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.986443</td>\n",
       "      <td>0.001410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.986115</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.986025</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.985072</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.987249</td>\n",
       "      <td>0.001270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2100</td>\n",
       "      <td>0.985990</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.985396</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.985468</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.987106</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.987456</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.987475</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.986703</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.988191</td>\n",
       "      <td>0.000946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.987636</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.986873</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.987487</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.988549</td>\n",
       "      <td>0.001123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_size  pooled_auc  pooled_auc_std  auc_mean_glioma  auc_std_glioma  \\\n",
       "0            25    0.712994        0.098449         0.645773        0.072372   \n",
       "1            50    0.835223        0.068722         0.773859        0.072240   \n",
       "2            75    0.899991        0.034488         0.897357        0.032403   \n",
       "3           100    0.928525        0.018110         0.934140        0.016937   \n",
       "4           150    0.948871        0.012130         0.954875        0.011957   \n",
       "5           200    0.958270        0.010108         0.966532        0.005436   \n",
       "6           250    0.964613        0.007218         0.970602        0.004577   \n",
       "7           300    0.967522        0.006599         0.969216        0.004773   \n",
       "8           400    0.973507        0.005944         0.977039        0.003113   \n",
       "9           500    0.975880        0.003988         0.977130        0.002884   \n",
       "10          600    0.977754        0.004787         0.981096        0.002151   \n",
       "11          800    0.980467        0.003636         0.982575        0.002143   \n",
       "12         1000    0.982947        0.002630         0.983436        0.001844   \n",
       "13         1200    0.983998        0.002702         0.984504        0.001507   \n",
       "14         1500    0.984945        0.001989         0.984715        0.001501   \n",
       "15         1800    0.986115        0.002018         0.986025        0.001334   \n",
       "16         2100    0.985990        0.001829         0.985396        0.001128   \n",
       "17         2500    0.987456        0.001593         0.987475        0.000984   \n",
       "18         3000    0.987636        0.001629         0.986873        0.000991   \n",
       "\n",
       "    auc_mean_meningioma  auc_std_glioma  auc_mean_metastasis  \\\n",
       "0              0.800798        0.072372             0.692411   \n",
       "1              0.891660        0.072240             0.840152   \n",
       "2              0.913687        0.032403             0.888931   \n",
       "3              0.927562        0.016937             0.923872   \n",
       "4              0.942720        0.011957             0.949018   \n",
       "5              0.948886        0.005436             0.959393   \n",
       "6              0.957131        0.004577             0.966105   \n",
       "7              0.962033        0.004773             0.971317   \n",
       "8              0.968108        0.003113             0.975375   \n",
       "9              0.971987        0.002884             0.978522   \n",
       "10             0.970941        0.002151             0.981225   \n",
       "11             0.977045        0.002143             0.981782   \n",
       "12             0.979853        0.001844             0.985551   \n",
       "13             0.981602        0.001507             0.985886   \n",
       "14             0.983677        0.001501             0.986443   \n",
       "15             0.985072        0.001334             0.987249   \n",
       "16             0.985468        0.001128             0.987106   \n",
       "17             0.986703        0.000984             0.988191   \n",
       "18             0.987487        0.000991             0.988549   \n",
       "\n",
       "    auc_std_metastasis  \n",
       "0             0.110357  \n",
       "1             0.081601  \n",
       "2             0.039001  \n",
       "3             0.019718  \n",
       "4             0.011717  \n",
       "5             0.011541  \n",
       "6             0.007668  \n",
       "7             0.005340  \n",
       "8             0.005917  \n",
       "9             0.003370  \n",
       "10            0.003265  \n",
       "11            0.002548  \n",
       "12            0.001871  \n",
       "13            0.001783  \n",
       "14            0.001410  \n",
       "15            0.001270  \n",
       "16            0.001211  \n",
       "17            0.000946  \n",
       "18            0.001123  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path_final_results = ### ENTER SOURCE DIRECTORY ###\n",
    "df_results_lasso = pd.read_excel(os.path.join(path_final_results, 'pooled_final_results_lasso.xlsx'))\n",
    "\n",
    "df_results_lasso[['sample_size', 'pooled_auc','pooled_auc_std', 'auc_mean_glioma', 'auc_std_glioma', 'auc_mean_meningioma', 'auc_std_glioma', 'auc_mean_metastasis', 'auc_std_metastasis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3G. Get results logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>pooled_auc</th>\n",
       "      <th>pooled_auc_std</th>\n",
       "      <th>auc_mean_glioma</th>\n",
       "      <th>auc_std_glioma</th>\n",
       "      <th>auc_mean_meningioma</th>\n",
       "      <th>auc_std_glioma</th>\n",
       "      <th>auc_mean_metastasis</th>\n",
       "      <th>auc_std_metastasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.509732</td>\n",
       "      <td>0.025537</td>\n",
       "      <td>0.515867</td>\n",
       "      <td>0.031513</td>\n",
       "      <td>0.505776</td>\n",
       "      <td>0.031513</td>\n",
       "      <td>0.507555</td>\n",
       "      <td>0.024826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.570702</td>\n",
       "      <td>0.101360</td>\n",
       "      <td>0.531664</td>\n",
       "      <td>0.041959</td>\n",
       "      <td>0.627722</td>\n",
       "      <td>0.041959</td>\n",
       "      <td>0.552719</td>\n",
       "      <td>0.052011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>0.681322</td>\n",
       "      <td>0.099870</td>\n",
       "      <td>0.643186</td>\n",
       "      <td>0.080922</td>\n",
       "      <td>0.792921</td>\n",
       "      <td>0.080922</td>\n",
       "      <td>0.607859</td>\n",
       "      <td>0.053303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.784374</td>\n",
       "      <td>0.083353</td>\n",
       "      <td>0.701061</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>0.886266</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>0.765797</td>\n",
       "      <td>0.113163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.869661</td>\n",
       "      <td>0.042154</td>\n",
       "      <td>0.832498</td>\n",
       "      <td>0.042225</td>\n",
       "      <td>0.907103</td>\n",
       "      <td>0.042225</td>\n",
       "      <td>0.869381</td>\n",
       "      <td>0.054945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.914377</td>\n",
       "      <td>0.022744</td>\n",
       "      <td>0.930762</td>\n",
       "      <td>0.024465</td>\n",
       "      <td>0.922036</td>\n",
       "      <td>0.024465</td>\n",
       "      <td>0.890333</td>\n",
       "      <td>0.024185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>0.932040</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.951406</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.934822</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.909892</td>\n",
       "      <td>0.011508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>0.938912</td>\n",
       "      <td>0.012552</td>\n",
       "      <td>0.955914</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.936237</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.924583</td>\n",
       "      <td>0.016552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400</td>\n",
       "      <td>0.952147</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>0.964059</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>0.946910</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>0.945474</td>\n",
       "      <td>0.014719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>0.961622</td>\n",
       "      <td>0.006677</td>\n",
       "      <td>0.971226</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.952100</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.961539</td>\n",
       "      <td>0.008441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>600</td>\n",
       "      <td>0.966481</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.973178</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>0.955254</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>0.971011</td>\n",
       "      <td>0.004612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>800</td>\n",
       "      <td>0.971960</td>\n",
       "      <td>0.003629</td>\n",
       "      <td>0.976831</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.962633</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.976417</td>\n",
       "      <td>0.002677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.975242</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>0.979064</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.968305</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.978358</td>\n",
       "      <td>0.002047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.978539</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0.981236</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.972501</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.002305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.982979</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.977232</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.983481</td>\n",
       "      <td>0.001550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.983302</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.983944</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.980101</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>0.985860</td>\n",
       "      <td>0.001367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2100</td>\n",
       "      <td>0.984109</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.985034</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.981164</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.986131</td>\n",
       "      <td>0.001411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.985409</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.985417</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.983168</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.987643</td>\n",
       "      <td>0.001142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.986317</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.986047</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.984651</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.988255</td>\n",
       "      <td>0.000994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_size  pooled_auc  pooled_auc_std  auc_mean_glioma  auc_std_glioma  \\\n",
       "0            25    0.509732        0.025537         0.515867        0.031513   \n",
       "1            50    0.570702        0.101360         0.531664        0.041959   \n",
       "2            75    0.681322        0.099870         0.643186        0.080922   \n",
       "3           100    0.784374        0.083353         0.701061        0.068212   \n",
       "4           150    0.869661        0.042154         0.832498        0.042225   \n",
       "5           200    0.914377        0.022744         0.930762        0.024465   \n",
       "6           250    0.932040        0.010949         0.951406        0.009800   \n",
       "7           300    0.938912        0.012552         0.955914        0.008471   \n",
       "8           400    0.952147        0.009749         0.964059        0.004845   \n",
       "9           500    0.961622        0.006677         0.971226        0.003465   \n",
       "10          600    0.966481        0.005216         0.973178        0.003341   \n",
       "11          800    0.971960        0.003629         0.976831        0.002333   \n",
       "12         1000    0.975242        0.003365         0.979064        0.002183   \n",
       "13         1200    0.978539        0.003359         0.981236        0.001506   \n",
       "14         1500    0.981231        0.002300         0.982979        0.001313   \n",
       "15         1800    0.983302        0.001770         0.983944        0.001244   \n",
       "16         2100    0.984109        0.001575         0.985034        0.000856   \n",
       "17         2500    0.985409        0.001329         0.985417        0.000888   \n",
       "18         3000    0.986317        0.001293         0.986047        0.000745   \n",
       "\n",
       "    auc_mean_meningioma  auc_std_glioma  auc_mean_metastasis  \\\n",
       "0              0.505776        0.031513             0.507555   \n",
       "1              0.627722        0.041959             0.552719   \n",
       "2              0.792921        0.080922             0.607859   \n",
       "3              0.886266        0.068212             0.765797   \n",
       "4              0.907103        0.042225             0.869381   \n",
       "5              0.922036        0.024465             0.890333   \n",
       "6              0.934822        0.009800             0.909892   \n",
       "7              0.936237        0.008471             0.924583   \n",
       "8              0.946910        0.004845             0.945474   \n",
       "9              0.952100        0.003465             0.961539   \n",
       "10             0.955254        0.003341             0.971011   \n",
       "11             0.962633        0.002333             0.976417   \n",
       "12             0.968305        0.002183             0.978358   \n",
       "13             0.972501        0.001506             0.981880   \n",
       "14             0.977232        0.001313             0.983481   \n",
       "15             0.980101        0.001244             0.985860   \n",
       "16             0.981164        0.000856             0.986131   \n",
       "17             0.983168        0.000888             0.987643   \n",
       "18             0.984651        0.000745             0.988255   \n",
       "\n",
       "    auc_std_metastasis  \n",
       "0             0.024826  \n",
       "1             0.052011  \n",
       "2             0.053303  \n",
       "3             0.113163  \n",
       "4             0.054945  \n",
       "5             0.024185  \n",
       "6             0.011508  \n",
       "7             0.016552  \n",
       "8             0.014719  \n",
       "9             0.008441  \n",
       "10            0.004612  \n",
       "11            0.002677  \n",
       "12            0.002047  \n",
       "13            0.002305  \n",
       "14            0.001550  \n",
       "15            0.001367  \n",
       "16            0.001411  \n",
       "17            0.001142  \n",
       "18            0.000994  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path_final_results = ### ENTER SOURCE DIRECTORY ###\n",
    "df_results_logistic = pd.read_excel(os.path.join(path_final_results, 'pooled_final_results_logistic.xlsx'))\n",
    "\n",
    "df_results_logistic[['sample_size', 'pooled_auc','pooled_auc_std', 'auc_mean_glioma', 'auc_std_glioma', 'auc_mean_meningioma', 'auc_std_glioma', 'auc_mean_metastasis', 'auc_std_metastasis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4A. Load preprocessed reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "source_dir = ### ENTER SOURCE DIRECTORY ###\n",
    "df = pd.read_excel(os.path.join(source_dir, 'preprocessed_reports.xlsx')) \n",
    "print('Dimensions df are', df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B. Create an parameter grid deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In total, we tested 58 settings across 15 hyperparameters resulting in a hyperparameter space \n",
    "# that encompasses 1,300,561,920 hypothetical model architectures.\n",
    "# Because it is computationally impossible to evaluate all these model architectures, \n",
    "# we applied an hierarchical approach for hyperparameter optimization by subsequently optimizing the:\n",
    "# 1) preprocessing settings (embedding_dim, max_len, max_words)\n",
    "# 2) network architecture (double_layer, filter_size, n_filters_conv, n_filters_dense, n_layers_conv, n_layers_dense)\n",
    "# 3) regularizers (dropout, l1, l2, max_pooling)\n",
    "# 4) learning speed (optimizer_type, learning_rate)\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid_dict = { \n",
    "    'double_layer': [0,1], \n",
    "    'dropout': [0, 0.1, 0.2, 0.5], \n",
    "    'embedding_dim': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'filter_size': [1,3,5,7,9], \n",
    "    'l1': [0.00000, 0.00001, 0.0001, 0.001],\n",
    "    'l2': [0.00000, 0.00001, 0.0001, 0.001], \n",
    "    'learning_rate': [0.002, 0.005, 0.01, 0.05], \n",
    "    'max_len': [250, 500, 1000, 2000],\n",
    "    'max_pooling' : [0,1],\n",
    "    'max_words': [250, 500, 750, 1000, 1500, 2000, 5000],\n",
    "    'n_filters_conv:': [8, 16, 32, 64],\n",
    "    'n_filters_dense':[8,16,32],\n",
    "    'n_layers_conv': [0,1,2,3],\n",
    "    'n_layers_dense':[0,1],\n",
    "    'optimizer_type': ['adam', 'nadam'],\n",
    "    'sample_size': [25, 50, 100, 150, 200, 250, 300, 400, 500, 750, 1000, 1500, 2000, 3000],\n",
    "                  }\n",
    "\n",
    "param_grid_list = list(ParameterGrid(param_grid_dict))\n",
    "\n",
    "parameter_grid = []\n",
    "for dict in param_grid_list:\n",
    "    hyperparameters = list(dict.values())\n",
    "    parameter_grid.append(hyperparameters)\n",
    "\n",
    "print('Length parameter grid is', len(parameter_grid), 'hyperparameter settings')\n",
    "\n",
    "parameter_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4C. Hyperparameter tuning deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this code block, two functions were used to automatically construct and evaluate different model architectures.\n",
    "### Running this code block on a multiple cores or a cloud service might be desirable.\n",
    "### Due to the hierarchical approach of the hyperparameter optimization,\n",
    "### the best model architecture is not automatically extracted \n",
    "### but retrieved through and iterative proces.\n",
    "\n",
    "from keras.layers import Embedding, Flatten, Dense, Input, LSTM, Conv1D, Dropout, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras import preprocessing\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "bootstrap_time_column = []\n",
    "n_layers_dense_column = []\n",
    "double_layer_column = []\n",
    "dropout_column = []\n",
    "embedding_dim_column = []\n",
    "filter_size_column = []\n",
    "l1_column = []\n",
    "l2_column = []\n",
    "learning_rate_column = []\n",
    "max_len_column = []\n",
    "max_pooling_column = []\n",
    "max_words_column = []\n",
    "n_filters_dense_column = []\n",
    "n_filters_conv_column = []\n",
    "n_layers_conv_column = []\n",
    "n_params_column = []\n",
    "optimizer_type_column = []\n",
    "sample_size_column = []\n",
    "auc_mean = []\n",
    "auc_std = []\n",
    "epochmeans = []\n",
    "epochstds = []\n",
    "\n",
    "gridsearch = 1\n",
    "\n",
    "\n",
    "def get_conv1d_model(n_layers_conv, double_layer, n_filters_conv, filter_size, l1, l2, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "    if n_layers_conv == 0:\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    elif n_layers_conv == 1:\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    elif n_layers_conv == 2:\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))            \n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    elif n_layers_conv == 3:\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))             \n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv*4, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv*4, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv*4, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(dropout))\n",
    "    if n_layers_conv == 1:\n",
    "        model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "    if max_pooling == 1:\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    if n_layers_dense == 1:\n",
    "        model.add(Dense(n_filters_dense, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=final_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def square(list):\n",
    "    return [i ** 2 for i in list]\n",
    "\n",
    "df_train_val = df[df['cohort'].isin(['train', 'validation'])]\n",
    "\n",
    "for double_layer, dropout, embedding_dim, filter_size, l1, l2, learning_rate, max_len, max_pooling, max_words, n_filters_conv, n_filters_dense, n_layers_conv, n_layers_dense, optimizer_type, sample_size in parameter_grid:\n",
    "    \n",
    "    print('##### Grid Search', gridsearch,'/', len(parameter_grid))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(df_train_val['stemmed_report'])\n",
    "    sequences = tokenizer.texts_to_sequences(df_train_val['stemmed_report'])\n",
    "    \n",
    "    df_train_val['vectorized_report'] = sequences \n",
    "    \n",
    "    bootstrap_aucs = []\n",
    "    bootstrap_auc_stds = []\n",
    "    \n",
    "    bootstrap_epoch = []\n",
    "    bootstrap_epoch_std = []\n",
    "    \n",
    "    bootstrap = 1\n",
    "\n",
    "    if optimizer_type == 'adam':\n",
    "        final_optimizer = optimizers.Adam(lr=learning_rate)\n",
    "    elif optimizer_type == 'nadam':\n",
    "        final_optimizer = optimizers.Nadam(lr=learning_rate)\n",
    "    \n",
    "    #model = get_conv1d_model(n_layers_conv, double_layer, n_filters_conv, filter_size, l1, l2, dropout)\n",
    "    model = get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer)\n",
    "    model.summary()\n",
    "\n",
    "    start_time = time.time()\n",
    "          \n",
    "    for n in range(int(3000//sample_size)): \n",
    "        \n",
    "        print('--- Bootstrap', bootstrap)\n",
    "\n",
    "        df_frac = df_train_val[(df_train_val['cohort'] == 'train')].sample(n = 2*sample_size, replace = True)\n",
    "\n",
    "        array_train = array(list(df_frac['vectorized_report'][(df_frac['cohort'] == 'train')]))\n",
    "        array_val = array(list(df_train_val['vectorized_report'][(df_train_val['cohort'] == 'validation')]))\n",
    "        X_train = preprocessing.sequence.pad_sequences(array_train, maxlen=max_len)\n",
    "        X_val = preprocessing.sequence.pad_sequences(array_val, maxlen=max_len) \n",
    "        \n",
    "        y_train = array(df_frac[['Glioma', 'Meningioma', 'Metastasis']][(df_frac['cohort'] == 'train')])\n",
    "        y_val = array(df_train_val[['Glioma', 'Meningioma', 'Metastasis']][(df_train_val['cohort'] == 'validation')])\n",
    "        \n",
    "        #model = get_conv1d_model(n_layers_conv, double_layer, n_filters_conv, filter_size, l1, l2, dropout)\n",
    "        model = get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer)\n",
    "        \n",
    "        el_cb = EarlyStopping(patience=20)\n",
    "        \n",
    "        try:\n",
    "            history = model.fit(X_train, y_train,\n",
    "                epochs=5000,\n",
    "                batch_size=32,\n",
    "                validation_split=0.5,\n",
    "                callbacks=[el_cb],\n",
    "                verbose = 0)\n",
    "            \n",
    "            auc = roc_auc_score(y_val, model.predict_proba(X_val))\n",
    "            n_epochs = len(history.epoch)\n",
    "        except ValueError:\n",
    "                pass\n",
    "        \n",
    "        print('AUC:', str(round(auc,3)))\n",
    "        bootstrap_aucs.append(auc)\n",
    "\n",
    "        print('n epochs:', str(round(n_epochs,3)))\n",
    "        bootstrap_epoch.append(n_epochs)\n",
    "        \n",
    "        bootstrap += 1\n",
    "\n",
    "    time_5_bootstraps = time.time() - start_time\n",
    "    \n",
    "    n_bootstraps = bootstrap - 1\n",
    "    \n",
    "    print('### Number of bootstraps is', n_bootstraps)\n",
    "    \n",
    "    bootstrap_auc_mean = np.mean(bootstrap_aucs)\n",
    "    bootstrap_auc_std = np.std(bootstrap_aucs)\n",
    "    \n",
    "    print('### Mean AUC for grid search', gridsearch, ':', round(bootstrap_auc_mean,2),'±', round(bootstrap_auc_std,3))\n",
    "    \n",
    "    bootstrap_epoch_mean = np.mean(bootstrap_epoch)\n",
    "    bootstrap_epoch_std = np.std(bootstrap_epoch)\n",
    "    \n",
    "    print('### Mean n epochs for grid search', gridsearch, ':', round(bootstrap_epoch_mean,2),'±', round(bootstrap_epoch_std,3))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    bootstrap_time_column.append(time_5_bootstraps)\n",
    "    double_layer_column.append(double_layer)\n",
    "    dropout_column.append(dropout)\n",
    "    embedding_dim_column.append(embedding_dim)\n",
    "    filter_size_column.append(filter_size)\n",
    "    l1_column.append(l1)\n",
    "    l2_column.append(l2)\n",
    "    learning_rate_column.append(learning_rate)\n",
    "    max_len_column.append(max_len)\n",
    "    max_pooling_column.append(max_pooling)\n",
    "    max_words_column.append(max_words)\n",
    "    n_filters_dense_column.append(n_filters_dense)\n",
    "    n_filters_conv_column.append(n_filters_conv)\n",
    "    n_layers_conv_column.append(n_layers_conv)\n",
    "    n_layers_dense_column.append(n_layers_dense)\n",
    "    n_params_column.append(model.count_params())\n",
    "    optimizer_type_column.append(optimizer_type)\n",
    "    sample_size_column.append(sample_size)\n",
    "    auc_mean.append(bootstrap_auc_mean)\n",
    "    auc_std.append(bootstrap_auc_std)\n",
    "    epochmeans.append(bootstrap_epoch_mean)\n",
    "    epochstds.append(bootstrap_epoch_std)\n",
    "    \n",
    "    gridsearch += 1\n",
    "\n",
    "    \n",
    "df_performance = pd.DataFrame(list(zip(sample_size_column, embedding_dim_column, max_len_column, max_words_column, \n",
    "                                       n_layers_conv_column, double_layer_column, n_filters_conv_column, filter_size_column,\n",
    "                                       max_pooling_column, n_layers_dense_column, n_filters_dense_column,\n",
    "                                       l1_column, l2_column, dropout_column, n_params_column,\n",
    "                                       auc_mean, auc_std, epochmeans, epochstds, optimizer_type_column, learning_rate_column, bootstrap_time_column)))\n",
    "\n",
    "\n",
    "df_performance.columns = ['sample_size', 'embedding_dim', 'max_len', 'max_words',\n",
    "                         'n_layers_conv', 'double_layer', 'n_filters_conv', 'filter_size',\n",
    "                         'max_pooling', 'n_layers_dense', 'n_filters_dense',\n",
    "                         'l1_column', 'l2_column', 'dropout', 'n_params',\n",
    "                         'auc_mean', 'auc_std', 'epochmeans', 'epochstds',\n",
    "                         'optimizer_type_column', 'learning_rate_column', 'bootstrap_time_column']\n",
    "\n",
    "target_dir = ### ENTER TARGET DIRECTORY ###\n",
    "\n",
    "df_performance.to_excel(os.path.join(target_dir, 'hyperparam_tuning_cnn.xlsx'))\n",
    "\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4D. Create optimal parametergrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Because the bootstrapped predictions on the final test required significant computational time,\n",
    "### the results were computed in 4 batches based on the sample size and merged together.\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid_dict = { \n",
    "    'double_layer': [0], #<- final\n",
    "    'dropout': [0.5], #<- final\n",
    "    'embedding_dim': [128],# <- final\n",
    "    'filter_size': [3], #<- final \n",
    "    'l1': [0],#<- final\n",
    "    'l2': [0], #<- final\n",
    "    'learning_rate': [0.01], #<- final\n",
    "    'max_len': [1000],#<- final\n",
    "    'max_pooling' : [1],#<- final\n",
    "    'max_words': [1000],#<- final\n",
    "    'n_filters_conv:': [0],#<- final\n",
    "    'n_filters_dense':[0],#<- final\n",
    "    'n_layers_conv': [0],#<- final\n",
    "    'n_layers_dense':[0],#<- final\n",
    "    'optimizer_type': ['adam'],\n",
    "    'sample_size': [25, 50, 100, 150, 200, 250, 300, 400, 500, 750, 1000, 1500, 2000, 3000],\n",
    "                  }\n",
    "\n",
    "param_grid_list = list(ParameterGrid(param_grid_dict))\n",
    "\n",
    "parameter_grid = []\n",
    "for dict in param_grid_list:\n",
    "    hyperparameters = list(dict.values())\n",
    "    parameter_grid.append(hyperparameters)\n",
    "\n",
    "print('Length parameter grid is', len(parameter_grid), 'hyperparameter settings')\n",
    "\n",
    "parameter_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4E. Create final performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both the CNN and non-CNN model architecture can be evaluated by specifying the get_conv1d_model2 function\n",
    "\n",
    "from keras.layers import Embedding, Flatten, Dense, Input, LSTM, Conv1D, Dropout, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras import preprocessing\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "bootstrap_time_column = []\n",
    "n_layers_dense_column = []\n",
    "double_layer_column = []\n",
    "dropout_column = []\n",
    "embedding_dim_column = []\n",
    "filter_size_column = []\n",
    "l1_column = []\n",
    "l2_column = []\n",
    "learning_rate_column = []\n",
    "max_len_column = []\n",
    "max_pooling_column = []\n",
    "max_words_column = []\n",
    "n_filters_dense_column = []\n",
    "n_filters_conv_column = []\n",
    "n_layers_conv_column = []\n",
    "n_params_column = []\n",
    "optimizer_type_column = []\n",
    "sample_size_column = []\n",
    "auc_mean = []\n",
    "auc_std = []\n",
    "epochmeans = []\n",
    "epochstds = []\n",
    "\n",
    "gridsearch = 1\n",
    "\n",
    "def get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(dropout))\n",
    "    if n_layers_conv == 1:\n",
    "        model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "    if max_pooling == 1:\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    if n_layers_dense == 1:\n",
    "        model.add(Dense(n_filters_dense, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=final_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def square(list):\n",
    "    return [i ** 2 for i in list]\n",
    "\n",
    "for double_layer, dropout, embedding_dim, filter_size, l1, l2, learning_rate, max_len, max_pooling, max_words, n_filters_conv, n_filters_dense, n_layers_conv, n_layers_dense, optimizer_type, sample_size in parameter_grid:\n",
    "    \n",
    "    print('##### Grid Search', gridsearch,'/', len(parameter_grid))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(df['stemmed_report'])\n",
    "    sequences = tokenizer.texts_to_sequences(df['stemmed_report'])\n",
    "    \n",
    "    df['vectorized_report'] = sequences \n",
    "    \n",
    "    bootstrap_aucs = []\n",
    "    bootstrap_auc_stds = []\n",
    "\n",
    "    \n",
    "    bootstrap = 1\n",
    "\n",
    "    if optimizer_type == 'adam':\n",
    "        final_optimizer = optimizers.Adam(lr=learning_rate)\n",
    "    elif optimizer_type == 'nadam':\n",
    "        final_optimizer = optimizers.Nadam(lr=learning_rate)\n",
    "    \n",
    "    model = get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer)\n",
    "    model.summary()\n",
    "\n",
    "    start_time = time.time()\n",
    "          \n",
    "    for n in range(100):\n",
    "        \n",
    "        print('--- Bootstrap', bootstrap)\n",
    "\n",
    "        df_frac = df[(df['cohort'] == 'train')].sample(n = 2*sample_size, replace = True)\n",
    "\n",
    "        array_train = array(list(df_frac['vectorized_report'][(df_frac['cohort'] == 'train')]))\n",
    "        array_test = array(list(df['vectorized_report'][(df['cohort'] == 'test')]))\n",
    "\n",
    "        X_train = preprocessing.sequence.pad_sequences(array_train, maxlen=max_len)\n",
    "        X_test = preprocessing.sequence.pad_sequences(array_test, maxlen=max_len)\n",
    "        \n",
    "        y_train = array(df_frac[['Glioma', 'Meningioma', 'Metastasis']][(df_frac['cohort'] == 'train')])\n",
    "        y_test = array(df[['Glioma', 'Meningioma', 'Metastasis']][(df['cohort'] == 'test')])\n",
    "        \n",
    "        model = get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer)\n",
    "        \n",
    "        try:\n",
    "            history = model.fit(X_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                verbose = 0)\n",
    "            \n",
    "            auc = roc_auc_score(y_test, model.predict_proba(X_test))\n",
    "            n_epochs = len(history.epoch)\n",
    "        except ValueError:\n",
    "                pass\n",
    "        \n",
    "        print('AUC:', str(round(auc,3)))\n",
    "        bootstrap_aucs.append(auc)\n",
    "\n",
    "        print('n epochs:', str(round(n_epochs,3)))\n",
    "        bootstrap_epoch.append(n_epochs)\n",
    "        \n",
    "        bootstrap += 1\n",
    "\n",
    "    time_100_bootstraps = time.time() - start_time\n",
    "    \n",
    "    n_bootstraps = bootstrap - 1\n",
    "    print('### Number of bootstraps is', n_bootstraps)\n",
    "    \n",
    "    bootstrap_auc_mean = np.mean(bootstrap_aucs)\n",
    "    bootstrap_auc_std = np.std(bootstrap_aucs)\n",
    "    print('### Mean AUC for grid search', gridsearch, ':', round(bootstrap_auc_mean,2),'±', round(bootstrap_auc_std,3))\n",
    "\n",
    "    bootstrap_epoch_mean = np.mean(bootstrap_epoch)\n",
    "    bootstrap_epoch_std = np.std(bootstrap_epoch)\n",
    "    print('### Mean n epochs for grid search', gridsearch, ':', round(bootstrap_epoch_mean,2),'±', round(bootstrap_epoch_std,3))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    bootstrap_time_column.append(time_100_bootstraps)\n",
    "    double_layer_column.append(double_layer)\n",
    "    dropout_column.append(dropout)\n",
    "    embedding_dim_column.append(embedding_dim)\n",
    "    filter_size_column.append(filter_size)\n",
    "    l1_column.append(l1)\n",
    "    l2_column.append(l2)\n",
    "    learning_rate_column.append(learning_rate)\n",
    "    max_len_column.append(max_len)\n",
    "    max_pooling_column.append(max_pooling)\n",
    "    max_words_column.append(max_words)\n",
    "    n_filters_dense_column.append(n_filters_dense)\n",
    "    n_filters_conv_column.append(n_filters_conv)\n",
    "    n_layers_conv_column.append(n_layers_conv)\n",
    "    n_layers_dense_column.append(n_layers_dense)\n",
    "    n_params_column.append(model.count_params())\n",
    "    optimizer_type_column.append(optimizer_type)\n",
    "    sample_size_column.append(sample_size)\n",
    "    auc_mean.append(bootstrap_auc_mean)\n",
    "    auc_std.append(bootstrap_auc_std)\n",
    "    epochmeans.append(bootstrap_epoch_mean)\n",
    "    epochstds.append(bootstrap_epoch_std)\n",
    "    \n",
    "    gridsearch += 1\n",
    "\n",
    "    \n",
    "df_performance = pd.DataFrame(list(zip(sample_size_column, embedding_dim_column, max_len_column, max_words_column, \n",
    "                                       n_layers_conv_column, double_layer_column, n_filters_conv_column, filter_size_column,\n",
    "                                       max_pooling_column, n_layers_dense_column, n_filters_dense_column,\n",
    "                                       l1_column, l2_column, dropout_column, n_params_column,\n",
    "                                       auc_mean, auc_std,\n",
    "                                       epochmeans, epochstds, optimizer_type_column, learning_rate_column, bootstrap_time_column)))\n",
    "\n",
    "\n",
    "df_performance.columns = ['sample_size', 'embedding_dim', 'max_len', 'max_words',\n",
    "                         'n_layers_conv', 'double_layer', 'n_filters_conv', 'filter_size',\n",
    "                         'max_pooling', 'n_layers_dense', 'n_filters_dense',\n",
    "                         'l1_column', 'l2_column', 'dropout', 'n_params',\n",
    "                         'auc_mean', 'auc_std', 'epochmeans', 'epochstds',\n",
    "                         'optimizer_type_column', 'learning_rate_column', 'bootstrap_time_column']\n",
    "\n",
    "target_dir = ### ENTER TARGET DIRECTORY ###\n",
    "\n",
    "df_performance.to_excel(os.path.join(target_dir, 'hyperparam_tuning_cnn.xlsx'))\n",
    "\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4F. Merge final results of the classical CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>0.076423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.848347</td>\n",
       "      <td>0.061993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>0.881338</td>\n",
       "      <td>0.048086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.909558</td>\n",
       "      <td>0.036291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.932190</td>\n",
       "      <td>0.028194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.936257</td>\n",
       "      <td>0.024811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>0.943332</td>\n",
       "      <td>0.021247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>0.949005</td>\n",
       "      <td>0.019768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400</td>\n",
       "      <td>0.956537</td>\n",
       "      <td>0.017690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>0.957508</td>\n",
       "      <td>0.017359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>600</td>\n",
       "      <td>0.962003</td>\n",
       "      <td>0.014450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>800</td>\n",
       "      <td>0.965562</td>\n",
       "      <td>0.012070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.964998</td>\n",
       "      <td>0.014441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.968475</td>\n",
       "      <td>0.013404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.968952</td>\n",
       "      <td>0.012914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.969162</td>\n",
       "      <td>0.013070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2100</td>\n",
       "      <td>0.966984</td>\n",
       "      <td>0.014069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.967558</td>\n",
       "      <td>0.013763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.966705</td>\n",
       "      <td>0.014247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_size  auc_mean   auc_std\n",
       "0            25  0.740113  0.076423\n",
       "1            50  0.848347  0.061993\n",
       "2            75  0.881338  0.048086\n",
       "3           100  0.909558  0.036291\n",
       "4           150  0.932190  0.028194\n",
       "5           200  0.936257  0.024811\n",
       "6           250  0.943332  0.021247\n",
       "7           300  0.949005  0.019768\n",
       "8           400  0.956537  0.017690\n",
       "9           500  0.957508  0.017359\n",
       "10          600  0.962003  0.014450\n",
       "11          800  0.965562  0.012070\n",
       "12         1000  0.964998  0.014441\n",
       "13         1200  0.968475  0.013404\n",
       "14         1500  0.968952  0.012914\n",
       "15         1800  0.969162  0.013070\n",
       "16         2100  0.966984  0.014069\n",
       "17         2500  0.967558  0.013763\n",
       "18         3000  0.966705  0.014247"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Because the bootstrapped predictions on the final test required significant computational time,\n",
    "### the results were computed in 4 batches based on the sample size and merged together.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path_final_results = ### ENTER SOURCE DIRECTORY ###\n",
    "df_results_cnn = pd.read_excel(os.path.join(path_final_results, 'part_1.xlsx'))\n",
    "df_results_part_1 = pd.read_excel(os.path.join(path_final_results, 'part_1.xlsx'))\n",
    "df_results_part_2 = pd.read_excel(os.path.join(path_final_results, 'part_2.xlsx'))\n",
    "df_results_part_3 = pd.read_excel(os.path.join(path_final_results, 'part_3.xlsx'))\n",
    "df_results_part_4 = pd.read_excel(os.path.join(path_final_results, 'part_4.xlsx'))\n",
    "\n",
    "df_results_cnn = df_results_part_1.append(df_results_part_2, ignore_index=True)\n",
    "df_results_cnn = df_results_cnn.append(df_results_part_3, ignore_index=True)\n",
    "df_results_cnn = df_results_cnn.append(df_results_part_4, ignore_index=True)\n",
    "\n",
    "df_results_cnn = df_results_cnn[['sample_size', 'auc_mean', 'auc_std']]\n",
    "\n",
    "df_results_cnn.to_excel(os.path.join(path_final_results, 'final_results.xlsx'))\n",
    "df_results_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4G. Merge final results of the ClinicalTextMiner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.806184</td>\n",
       "      <td>0.049835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.904739</td>\n",
       "      <td>0.030762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>0.944951</td>\n",
       "      <td>0.020864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.961468</td>\n",
       "      <td>0.013930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.977225</td>\n",
       "      <td>0.007986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.982019</td>\n",
       "      <td>0.004516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>0.984145</td>\n",
       "      <td>0.002987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>0.986416</td>\n",
       "      <td>0.002341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400</td>\n",
       "      <td>0.987951</td>\n",
       "      <td>0.001850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>0.988919</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>600</td>\n",
       "      <td>0.989720</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>800</td>\n",
       "      <td>0.990205</td>\n",
       "      <td>0.000895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.990648</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.991106</td>\n",
       "      <td>0.000649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.991380</td>\n",
       "      <td>0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.000650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2100</td>\n",
       "      <td>0.991745</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.991981</td>\n",
       "      <td>0.000542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.992086</td>\n",
       "      <td>0.000520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_size  auc_mean   auc_std\n",
       "0            25  0.806184  0.049835\n",
       "1            50  0.904739  0.030762\n",
       "2            75  0.944951  0.020864\n",
       "3           100  0.961468  0.013930\n",
       "4           150  0.977225  0.007986\n",
       "5           200  0.982019  0.004516\n",
       "6           250  0.984145  0.002987\n",
       "7           300  0.986416  0.002341\n",
       "8           400  0.987951  0.001850\n",
       "9           500  0.988919  0.001343\n",
       "10          600  0.989720  0.001153\n",
       "11          800  0.990205  0.000895\n",
       "12         1000  0.990648  0.000796\n",
       "13         1200  0.991106  0.000649\n",
       "14         1500  0.991380  0.000549\n",
       "15         1800  0.991597  0.000650\n",
       "16         2100  0.991745  0.000602\n",
       "17         2500  0.991981  0.000542\n",
       "18         3000  0.992086  0.000520"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Because the bootstrapped predictions on the final test required significant computational time,\n",
    "### the results were computed in 4 batches based on the sample size and merged together.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path_final_results = ### ENTER SOURCE DIRECTORY ###\n",
    "df_results_embed = pd.read_excel(os.path.join(path_final_results, 'part_1.xlsx'))\n",
    "df_results_part_1 = pd.read_excel(os.path.join(path_final_results, 'part_1.xlsx'))\n",
    "df_results_part_2 = pd.read_excel(os.path.join(path_final_results, 'part_2.xlsx'))\n",
    "df_results_part_3 = pd.read_excel(os.path.join(path_final_results, 'part_3.xlsx'))\n",
    "df_results_part_4 = pd.read_excel(os.path.join(path_final_results, 'part_4.xlsx'))\n",
    "\n",
    "df_results_embed = df_results_part_1.append(df_results_part_2, ignore_index=True)\n",
    "df_results_embed = df_results_embed.append(df_results_part_3, ignore_index=True)\n",
    "df_results_embed = df_results_embed.append(df_results_part_4, ignore_index=True)\n",
    "\n",
    "df_results_embed = df_results_embed[['sample_size', 'auc_mean', 'auc_std']]\n",
    "\n",
    "df_results_embed.to_excel(os.path.join(path_final_results, 'final_results.xlsx'))\n",
    "df_results_embed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
