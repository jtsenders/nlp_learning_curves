{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Example format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This dummy dataframe depicts the format of the input data\n",
    "### The labels are are provided in a one-hot encoded fashion and as such mutually exclusive\n",
    "import pandas as pd\n",
    "\n",
    "d = {'Variable_1': [0, 0, 0, 1, 1, 0, 0, 0, 0, 0], \n",
    "     'Variable_2': [0, 1, 0, 0, 0, 1, 0, 1, 0, 1],\n",
    "     'Variable_3': [1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n",
    "     'Report': ['free text']*10}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set directories\n",
    "source_dir = ### INSERT SOURCE DIRECTORY ###\n",
    "\n",
    "\n",
    "# Import Excel file\n",
    "df = pd.read_excel(os.path.join(source_dir,'df_one_hot_encoded.xlsx'))\n",
    "\n",
    "df = df[['Glioma', 'Meningioma', 'Metastasis', 'Report', 'Patient_number']]\n",
    "df[['Glioma', 'Meningioma', 'Metastasis', 'Report', 'Patient_number']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A. Remove uneccesary patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    \n",
    "    #remove all text before 'CLINICAL DATA'\n",
    "    text = re.sub(r'.*CLINICAL DATA', 'CLINICAL DATA', text)\n",
    "    \n",
    "    #remove disclaimer and signature'\n",
    "    text = text.split('These tests were developed and their performance characteristics determined bythe Molecular Diagnostics Laboratory')[0]\n",
    "    \n",
    "    # Remove newlines\n",
    "    text = text.replace(r'\\n', ' ')\n",
    "    \n",
    "    # Remove date\n",
    "    date_pattern = r'[0-9]{1,2}[-/][0-9]{1,2}[-/][0-9]{2,}'\n",
    "    text = re.sub(date_pattern, ' ', text.lower())\n",
    "    \n",
    "    # Remove whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove punctution, keep decimal points\n",
    "    text = re.sub(r'[\\W]+(?!\\d)', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['cleaned_report'] = df['Report'].apply(preprocessor)\n",
    "df['cleaned_report'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "stop_words_modified = [w for w in stop_words_english if w not in ['no', 'not']]\n",
    "\n",
    "def remove_words(clean_report): \n",
    "    clean_report_words = clean_report.split()\n",
    "    stripped_report_words  = [word for word in clean_report_words if word.lower() not in stop_words_modified]\n",
    "    stripped_report_text = ' '.join(stripped_report_words)\n",
    "    return stripped_report_text\n",
    "\n",
    "df['stripped_report'] = df['cleaned_report'].apply(remove_words)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2C. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def porter_stemming(text):\n",
    "    stemmed_report_words = [porter.stem(word) for word in text.split()]\n",
    "    stemmed_report_text = ' '.join(stemmed_report_words)\n",
    "    return stemmed_report_text\n",
    "\n",
    "df['stemmed_report'] = df['stripped_report'].apply(porter_stemming)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(report): \n",
    "    report = report.split()\n",
    "    return report\n",
    "\n",
    "df['tokenized_report'] = df['stemmed_report'].apply(split_words)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2E.  Train validation test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "list_train, list_validation_test = train_test_split(df.Patient_number.unique(), test_size = 0.5)\n",
    "list_validation, list_test = train_test_split(list_validation_test, test_size = 0.5)\n",
    "print('n patients in training set:', len(list_train))\n",
    "print('n patients in validation set:', len(list_validation))\n",
    "print('n patients in test set:', len(list_test))\n",
    "\n",
    "df.loc[df['Patient_number'].isin(list_train), 'cohort'] = \"train\"\n",
    "df.loc[df['Patient_number'].isin(list_validation), 'cohort'] = \"validation\"\n",
    "df.loc[df['Patient_number'].isin(list_test), 'cohort'] = \"test\"\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2F. Save Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = ### INSERT TARGET DIRECTORY ###\n",
    "df.to_excel(os.path.join(target_dir, 'preprocessed_reports.xlsx')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2H. Load Preprocessed Reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "source_dir = ### INSERT SOURCE DIRECTORY ###\n",
    "df = pd.read_excel(os.path.join(source_dir, 'preprocessed_reports.xlsx')) \n",
    "print('Dimensions df are', df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Statistical / Classic Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Create an parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid_dict = {'diagnosis': ['Glioma', 'Meningioma', 'Metastasis'],\n",
    "                    'l1': [1, 2, 4, 8, 16, 32],\n",
    "                   'max_features':[100, 250, 500, 1000, 1500, 2000],\n",
    "                   'ngram_range': [1, 2, 3],\n",
    "                   'sample_size': [25, 50, 75, 100, 150, 200, 250, 300, 400, 500, 600, 800, 1000, 1200, 1500, 1800, 2100, 2500, 3000] \n",
    "                  }\n",
    "\n",
    "param_grid_list = list(ParameterGrid(param_grid_dict))\n",
    "param_grid_list\n",
    "\n",
    "parameter_grid = []\n",
    "for dict in param_grid_list:\n",
    "    hyperparameters = list(dict.values())\n",
    "    parameter_grid.append(hyperparameters)\n",
    "\n",
    "print('Length parameter grid is', len(parameter_grid), 'hyperparameter settings')\n",
    "parameter_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D. Hyperparameter tuning regression-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running this code block on a multiple cores or a cloud service might be desirable\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import linear_model\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "def square(list):\n",
    "    return [i ** 2 for i in list]\n",
    "\n",
    "diagnosis_column = []\n",
    "l1_column = []\n",
    "max_features_column = []\n",
    "ngram_range_column = []\n",
    "sample_size_column = []\n",
    "auc_mean_column = []\n",
    "auc_std_column = []\n",
    "number_of_bootstraps = []\n",
    "\n",
    "start_gridsearch = 1\n",
    "\n",
    "for diagnosis, l1, max_features, ngram_range, sample_size in parameter_grid:\n",
    "    \n",
    "    print('##### Grid Search', start_gridsearch,'/', len(parameter_grid))\n",
    "    \n",
    "    word_vectorizer = TfidfVectorizer(ngram_range=(1,ngram_range),max_features=max_features)\n",
    "    word_vectorizer.fit(df['tokenized_report'][df['cohort'].isin(['train', 'validation'])])\n",
    "  \n",
    "    bootstrap_aucs = []\n",
    "    \n",
    "    start_bootstrap = 1\n",
    "    \n",
    "    for n in range(int(3000//sample_size)):\n",
    "        \n",
    "        df_frac = df[(df['cohort'] == 'train')].sample(n = sample_size, replace = True)\n",
    "    \n",
    "        X_train = word_vectorizer.transform(df_frac['tokenized_report']).toarray()\n",
    "        y_train = np.asarray(df_frac[diagnosis])\n",
    "        \n",
    "        X_val = word_vectorizer.transform(df['tokenized_report'][df['cohort'].isin(['validation'])]).toarray()\n",
    "        y_val = np.asarray(df[diagnosis][df['cohort'].isin(['validation'])])\n",
    "        \n",
    "        try:\n",
    "            model_lasso = LogisticRegression(penalty = 'l1', C=l1, max_iter=200)\n",
    "            model_lasso.fit(X_train, y_train)\n",
    "\n",
    "            bootstrap_auc = roc_auc_score(y_val, model_lasso.predict_proba(X_val)[:,1])\n",
    "            bootstrap_aucs.append(bootstrap_auc)\n",
    "        \n",
    "        except ValueError:\n",
    "               pass\n",
    "                                            \n",
    "        start_bootstrap = start_bootstrap + 1 \n",
    "                                          \n",
    "    n_bootstraps = start_bootstrap - 1\n",
    "    \n",
    "    print('Number of bootstraps is', n_bootstraps)                                     \n",
    "                                          \n",
    "    auc_mean = np.mean(bootstrap_aucs)\n",
    "    auc_std = np.std(bootstrap_aucs)\n",
    "\n",
    "    print('The mean bootstrapped AUC for grid search', start_gridsearch, 'bootstrap ', start_bootstrap,\n",
    "          round(auc_mean,2),'±', round(auc_std,3))\n",
    "    \n",
    "    diagnosis_column.append(diagnosis)\n",
    "    l1_column.append(l1)\n",
    "    max_features_column.append(max_features)\n",
    "    ngram_range_column.append(ngram_range)\n",
    "    sample_size_column.append(sample_size)\n",
    "    auc_mean_column.append(auc_mean)\n",
    "    auc_std_column.append(auc_std)\n",
    "    number_of_bootstraps.append(n_bootstraps)\n",
    "    \n",
    "    start_gridsearch = start_gridsearch + 1\n",
    "    \n",
    "df_performance = pd.DataFrame(list(zip(diagnosis_column, sample_size_column, l1_column, max_features_column, ngram_range_column,\n",
    "                                    auc_mean_column, auc_std_column, number_of_bootstraps)))\n",
    "\n",
    "df_performance.columns = ['diagnosis', 'sample_size', 'l1', 'max_features', 'ngram_range',  'auc_mean', \n",
    "                          'auc_std', 'number_of_bootstraps']\n",
    "\n",
    "target_dir = ### INSERT TARGET DIRECTORY ###\n",
    "df_performance.to_excel(os.path.join(target_dir, 'hyperparameter_tuning.xlsx'))\n",
    "\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D. Select optimal hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "target_dir = ### INSERT SOURCE DIRECTORY ###\n",
    "df_hyperparameter_results = pd.read_excel(os.path.join(target_dir, 'iteration_1.xlsx'))\n",
    "\n",
    "df_hyperparameter_optimal_lasso = df_hyperparameter_results.loc[df_hyperparameter_results.reset_index().groupby(['diagnosis', 'sample_size'])['auc_mean'].idxmax()]\n",
    "df_hyperparameter_optimal_lasso = df_hyperparameter_optimal_lasso[['diagnosis', 'l1', 'max_features', 'ngram_range', 'sample_size']]\n",
    "optimal_paramater_grid_lasso = df_hyperparameter_optimal_lasso.values.tolist()\n",
    "optimal_paramater_grid_lasso\n",
    "\n",
    "df_hyperparameter_results_logistic = df_hyperparameter_results.loc[df_hyperparameter_results['l1'] == 1]\n",
    "df_hyperparameter_optimal_logistic = df_hyperparameter_results_logistic.loc[df_hyperparameter_results_logistic.groupby(['diagnosis', 'sample_size'])['auc_mean'].idxmax()]\n",
    "df_hyperparameter_optimal_logistic = df_hyperparameter_optimal_logistic[['diagnosis', 'l1', 'max_features', 'ngram_range', 'sample_size']]\n",
    "optimal_paramater_grid_logistic = df_hyperparameter_optimal_logistic.values.tolist()\n",
    "optimal_paramater_grid_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3E. Train final models and compute predicted probabilities and classses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this code block twice. Once for Lasso, once for logistic regression.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold, train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import linear_model\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def square(list):\n",
    "    return [i ** 2 for i in list]\n",
    "\n",
    "diagnosis_column = []\n",
    "l1_column = []\n",
    "max_features_column = []\n",
    "ngram_range_column = []\n",
    "sample_size_column = []\n",
    "auc_mean = []\n",
    "auc_std = []\n",
    "number_of_bootstraps = []\n",
    "\n",
    "gridsearch = 1\n",
    "\n",
    "for diagnosis, l1, max_features, ngram_range, sample_size in optimal_paramater_grid_logistic:\n",
    "    \n",
    "    print('##### Grid Search', gridsearch,'/', len(optimal_paramater_grid_lasso))\n",
    "    \n",
    "    word_vectorizer = TfidfVectorizer(ngram_range=(1,ngram_range),max_features=max_features)\n",
    "    word_vectorizer.fit(df['tokenized_report'][df['cohort'].isin(['train', 'test'])])\n",
    "\n",
    "    bootstrap_aucs = [] \n",
    "    \n",
    "    start_bootstrap = 1\n",
    "    \n",
    "    for n in range(100):\n",
    "        \n",
    "        df_frac = df[(df['cohort'] == 'train')].sample(n = sample_size, replace = True)\n",
    "    \n",
    "        X_train = word_vectorizer.transform(df_frac['tokenized_report']).toarray()\n",
    "        y_train = np.asarray(df_frac[diagnosis])\n",
    "        \n",
    "        X_test = word_vectorizer.transform(df['tokenized_report'][df['cohort'].isin(['test'])]).toarray()\n",
    "        y_test = np.asarray(df[diagnosis][df['cohort'].isin(['test'])])\n",
    "        \n",
    "        try:\n",
    "            model = LogisticRegression(penalty = 'l1', C=l1, max_iter=200)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            predicted_probabilities = model.predict_proba(X_test)[:,1]\n",
    "            auc = roc_auc_score(y_test, predicted_probabilities)\n",
    "\n",
    "        except ValueError:\n",
    "              pass\n",
    "        \n",
    "        print('AUC:', str(round(auc,3)))\n",
    "        bootstrap_aucs.append(auc)\n",
    "        \n",
    "        gridsearch += 1 \n",
    "                                          \n",
    "    n_bootstraps = gridsearch - 1\n",
    "    print('### Number of bootstraps is', n_bootstraps)\n",
    "    \n",
    "    bootstrap_auc_mean = np.mean(bootstrap_aucs)\n",
    "    bootstrap_auc_std = np.std(bootstrap_aucs)\n",
    "    print('### Mean AUC for grid search', gridsearch, ':', round(bootstrap_auc_mean,2),'±', round(bootstrap_auc_std,3))\n",
    "\n",
    "    print('')\n",
    "    \n",
    "    diagnosis_column.append(diagnosis)\n",
    "    l1_column.append(l1)\n",
    "    max_features_column.append(max_features)\n",
    "    ngram_range_column.append(ngram_range)\n",
    "    sample_size_column.append(sample_size)\n",
    "    auc_mean.append(bootstrap_auc_mean)\n",
    "    auc_std.append(bootstrap_auc_std)\n",
    "    number_of_bootstraps.append(n_bootstraps)\n",
    "    \n",
    "    start_gridsearch = start_gridsearch + 1\n",
    "    \n",
    "df_performance = pd.DataFrame(list(zip(diagnosis_column, sample_size_column, l1_column, max_features_column, ngram_range_column,\n",
    "                                    auc_mean, auc_std, number_of_bootstraps)))\n",
    "\n",
    "df_performance.columns = ['diagnosis', 'sample_size', 'l1', 'max_features', 'ngram_range',  'auc_mean', \n",
    "                          'auc_std', 'number_of_bootstraps']\n",
    "\n",
    "target_dir = ### INSERT SOURCE DIRECTORY ###\n",
    "\n",
    "df_performance.to_excel(os.path.join(target_dir, 'final_results_logistic.xlsx'))\n",
    "\n",
    "df_performance\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3F. Get results lasso regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>pooled_auc</th>\n",
       "      <th>pooled_auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.697872</td>\n",
       "      <td>0.105783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.850333</td>\n",
       "      <td>0.069648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>0.904126</td>\n",
       "      <td>0.033465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.929045</td>\n",
       "      <td>0.022178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.949481</td>\n",
       "      <td>0.014136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.958935</td>\n",
       "      <td>0.010419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>0.964847</td>\n",
       "      <td>0.008759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>0.968405</td>\n",
       "      <td>0.007763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400</td>\n",
       "      <td>0.973312</td>\n",
       "      <td>0.005666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>0.977467</td>\n",
       "      <td>0.003977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>600</td>\n",
       "      <td>0.980954</td>\n",
       "      <td>0.003314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>800</td>\n",
       "      <td>0.983158</td>\n",
       "      <td>0.003103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.985327</td>\n",
       "      <td>0.002639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.986449</td>\n",
       "      <td>0.002117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.987820</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.988590</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2100</td>\n",
       "      <td>0.989046</td>\n",
       "      <td>0.001513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.989865</td>\n",
       "      <td>0.001185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.990368</td>\n",
       "      <td>0.001213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_size  pooled_auc  pooled_auc_std\n",
       "0            25    0.697872        0.105783\n",
       "1            50    0.850333        0.069648\n",
       "2            75    0.904126        0.033465\n",
       "3           100    0.929045        0.022178\n",
       "4           150    0.949481        0.014136\n",
       "5           200    0.958935        0.010419\n",
       "6           250    0.964847        0.008759\n",
       "7           300    0.968405        0.007763\n",
       "8           400    0.973312        0.005666\n",
       "9           500    0.977467        0.003977\n",
       "10          600    0.980954        0.003314\n",
       "11          800    0.983158        0.003103\n",
       "12         1000    0.985327        0.002639\n",
       "13         1200    0.986449        0.002117\n",
       "14         1500    0.987820        0.001832\n",
       "15         1800    0.988590        0.001527\n",
       "16         2100    0.989046        0.001513\n",
       "17         2500    0.989865        0.001185\n",
       "18         3000    0.990368        0.001213"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path_final_results = ### INSERT SOURCE DIRECTORY ###\n",
    "df_results_lasso = pd.read_excel(os.path.join(path_final_results, 'pooled_final_results_lasso.xlsx'))\n",
    "df_results_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3G. Get results logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>pooled_auc</th>\n",
       "      <th>pooled_auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.506660</td>\n",
       "      <td>0.028843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.582606</td>\n",
       "      <td>0.108954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>0.715015</td>\n",
       "      <td>0.095543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.785471</td>\n",
       "      <td>0.078212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.890234</td>\n",
       "      <td>0.034359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.918495</td>\n",
       "      <td>0.019208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>0.928700</td>\n",
       "      <td>0.014242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>0.943737</td>\n",
       "      <td>0.010542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400</td>\n",
       "      <td>0.950385</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>0.961471</td>\n",
       "      <td>0.008590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>600</td>\n",
       "      <td>0.969116</td>\n",
       "      <td>0.005831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>800</td>\n",
       "      <td>0.973763</td>\n",
       "      <td>0.004583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.978897</td>\n",
       "      <td>0.003145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.981344</td>\n",
       "      <td>0.002256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.983698</td>\n",
       "      <td>0.001980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.985219</td>\n",
       "      <td>0.001594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2100</td>\n",
       "      <td>0.986458</td>\n",
       "      <td>0.001257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.987638</td>\n",
       "      <td>0.001133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.988514</td>\n",
       "      <td>0.001026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_size  pooled_auc  pooled_auc_std\n",
       "0            25    0.506660        0.028843\n",
       "1            50    0.582606        0.108954\n",
       "2            75    0.715015        0.095543\n",
       "3           100    0.785471        0.078212\n",
       "4           150    0.890234        0.034359\n",
       "5           200    0.918495        0.019208\n",
       "6           250    0.928700        0.014242\n",
       "7           300    0.943737        0.010542\n",
       "8           400    0.950385        0.008844\n",
       "9           500    0.961471        0.008590\n",
       "10          600    0.969116        0.005831\n",
       "11          800    0.973763        0.004583\n",
       "12         1000    0.978897        0.003145\n",
       "13         1200    0.981344        0.002256\n",
       "14         1500    0.983698        0.001980\n",
       "15         1800    0.985219        0.001594\n",
       "16         2100    0.986458        0.001257\n",
       "17         2500    0.987638        0.001133\n",
       "18         3000    0.988514        0.001026"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path_final_results = ### INSERT SOURCE DIRECTORY ###\n",
    "df_results_logistic = pd.read_excel(os.path.join(path_final_results, 'pooled_final_results_logistic.xlsx'))\n",
    "df_results_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4A. Load preprocessed reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "source_dir = ### INSERT SOURCE DIRECTORY ###\n",
    "df = pd.read_excel(os.path.join(source_dir, 'preprocessed_reports.xlsx')) \n",
    "print('Dimensions df are', df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B. Create a parameter grid deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In total, we tested 58 settings across 15 hyperparameters resulting in a hyperparameter space \n",
    "# that encompasses 1,300,561,920 hypothetical model architectures.\n",
    "# Because it is virtually impossible to evaluate all these model architectures from a computational standpoint, \n",
    "# we applied an hierarchical approach for hyperparameter optimization by subsequently optimizing the:\n",
    "# 1) preprocessing settings (embedding_dim, max_len, max_words)\n",
    "# 2) network architecture (double_layer, filter_size, n_filters_conv, n_filters_dense, n_layers_conv, n_layers_dense)\n",
    "# 3) regularizers (dropout, l1, l2, max_pooling)\n",
    "# 4) learning speed (optimizer_type, learning_rate)\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid_dict = { \n",
    "    'double_layer': [0,1], \n",
    "    'dropout': [0, 0.1, 0.2, 0.5], \n",
    "    'embedding_dim': [8, 16, 32, 64, 128, 256, 512],\n",
    "    'filter_size': [1,3,5,7,9], \n",
    "    'l1': [0.00000, 0.00001, 0.0001, 0.001],\n",
    "    'l2': [0.00000, 0.00001, 0.0001, 0.001], \n",
    "    'learning_rate': [0.002, 0.005, 0.01, 0.05], \n",
    "    'max_len': [250, 500, 1000, 2000],\n",
    "    'max_pooling' : [0,1],\n",
    "    'max_words': [250, 500, 750, 1000, 1500, 2000, 5000],\n",
    "    'n_filters_conv:': [8, 16, 32, 64],\n",
    "    'n_filters_dense':[8,16,32],\n",
    "    'n_layers_conv': [0,1,2,3],\n",
    "    'n_layers_dense':[0,1],\n",
    "    'optimizer_type': ['adam', 'nadam'],\n",
    "    'sample_size': [25, 50, 100, 150, 200, 250, 300, 400, 500, 750, 1000, 1500, 2000, 3000],\n",
    "                  }\n",
    "\n",
    "param_grid_list = list(ParameterGrid(param_grid_dict))\n",
    "\n",
    "parameter_grid = []\n",
    "for dict in param_grid_list:\n",
    "    hyperparameters = list(dict.values())\n",
    "    parameter_grid.append(hyperparameters)\n",
    "\n",
    "print('Length parameter grid is', len(parameter_grid), 'hyperparameter settings')\n",
    "\n",
    "parameter_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4C. Hyperparameter tuning deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this code block, two functions were used to automatically construct and evaluate different model architectures.\n",
    "### Running this code block on a multiple cores or a cloud service might be desirable.\n",
    "### Due to the hierarchical approach of the hyperparameter optimization,\n",
    "### the best model architecture is not automatically extracted \n",
    "### but retrieved through and iterative proces.\n",
    "\n",
    "from keras.layers import Embedding, Flatten, Dense, Input, LSTM, Conv1D, Dropout, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras import preprocessing\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "bootstrap_time_column = []\n",
    "n_layers_dense_column = []\n",
    "double_layer_column = []\n",
    "dropout_column = []\n",
    "embedding_dim_column = []\n",
    "filter_size_column = []\n",
    "l1_column = []\n",
    "l2_column = []\n",
    "learning_rate_column = []\n",
    "max_len_column = []\n",
    "max_pooling_column = []\n",
    "max_words_column = []\n",
    "n_filters_dense_column = []\n",
    "n_filters_conv_column = []\n",
    "n_layers_conv_column = []\n",
    "n_params_column = []\n",
    "optimizer_type_column = []\n",
    "sample_size_column = []\n",
    "auc_mean = []\n",
    "auc_std = []\n",
    "epochmeans = []\n",
    "epochstds = []\n",
    "\n",
    "gridsearch = 1\n",
    "\n",
    "\n",
    "def get_conv1d_model(n_layers_conv, double_layer, n_filters_conv, filter_size, l1, l2, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "    if n_layers_conv == 0:\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    elif n_layers_conv == 1:\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    elif n_layers_conv == 2:\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))            \n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    elif n_layers_conv == 3:\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv*2, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))             \n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "        if double_layer == 0:\n",
    "            model.add(layers.Conv1D(n_filters_conv*4, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        elif double_layer == 1:\n",
    "            model.add(layers.Conv1D(n_filters_conv*4, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "            model.add(layers.Conv1D(n_filters_conv*4, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(dropout))\n",
    "    if n_layers_conv == 1:\n",
    "        model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "    if max_pooling == 1:\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    if n_layers_dense == 1:\n",
    "        model.add(Dense(n_filters_dense, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=final_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def square(list):\n",
    "    return [i ** 2 for i in list]\n",
    "\n",
    "df_train_val = df[df['cohort'].isin(['train', 'validation'])]\n",
    "\n",
    "for double_layer, dropout, embedding_dim, filter_size, l1, l2, learning_rate, max_len, max_pooling, max_words, n_filters_conv, n_filters_dense, n_layers_conv, n_layers_dense, optimizer_type, sample_size in parameter_grid:\n",
    "    \n",
    "    print('##### Grid Search', gridsearch,'/', len(parameter_grid))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(df_train_val['stemmed_report'])\n",
    "    sequences = tokenizer.texts_to_sequences(df_train_val['stemmed_report'])\n",
    "    \n",
    "    df_train_val['vectorized_report'] = sequences \n",
    "    \n",
    "    bootstrap_aucs = []\n",
    "    bootstrap_auc_stds = []\n",
    "    \n",
    "    bootstrap_epoch = []\n",
    "    bootstrap_epoch_std = []\n",
    "    \n",
    "    bootstrap = 1\n",
    "\n",
    "    if optimizer_type == 'adam':\n",
    "        final_optimizer = optimizers.Adam(lr=learning_rate)\n",
    "    elif optimizer_type == 'nadam':\n",
    "        final_optimizer = optimizers.Nadam(lr=learning_rate)\n",
    "    \n",
    "    #model = get_conv1d_model(n_layers_conv, double_layer, n_filters_conv, filter_size, l1, l2, dropout)\n",
    "    model = get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer)\n",
    "    model.summary()\n",
    "\n",
    "    start_time = time.time()\n",
    "          \n",
    "    for n in range(int(3000//sample_size)): \n",
    "        \n",
    "        print('--- Bootstrap', bootstrap)\n",
    "\n",
    "        df_frac = df_train_val[(df_train_val['cohort'] == 'train')].sample(n = 2*sample_size, replace = True)\n",
    "\n",
    "        array_train = array(list(df_frac['vectorized_report'][(df_frac['cohort'] == 'train')]))\n",
    "        array_val = array(list(df_train_val['vectorized_report'][(df_train_val['cohort'] == 'validation')]))\n",
    "        X_train = preprocessing.sequence.pad_sequences(array_train, maxlen=max_len)\n",
    "        X_val = preprocessing.sequence.pad_sequences(array_val, maxlen=max_len) \n",
    "        \n",
    "        y_train = array(df_frac[['Glioma', 'Meningioma', 'Metastasis']][(df_frac['cohort'] == 'train')])\n",
    "        y_val = array(df_train_val[['Glioma', 'Meningioma', 'Metastasis']][(df_train_val['cohort'] == 'validation')])\n",
    "        \n",
    "        #model = get_conv1d_model(n_layers_conv, double_layer, n_filters_conv, filter_size, l1, l2, dropout)\n",
    "        model = get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer)\n",
    "        \n",
    "        el_cb = EarlyStopping(patience=20)\n",
    "        \n",
    "        try:\n",
    "            history = model.fit(X_train, y_train,\n",
    "                epochs=5000,\n",
    "                batch_size=32,\n",
    "                validation_split=0.5,\n",
    "                callbacks=[el_cb],\n",
    "                verbose = 0)\n",
    "            \n",
    "            auc = roc_auc_score(y_val, model.predict_proba(X_val))\n",
    "            n_epochs = len(history.epoch)\n",
    "        except ValueError:\n",
    "                pass\n",
    "        \n",
    "        print('AUC:', str(round(auc,3)))\n",
    "        bootstrap_aucs.append(auc)\n",
    "\n",
    "        print('n epochs:', str(round(n_epochs,3)))\n",
    "        bootstrap_epoch.append(n_epochs)\n",
    "        \n",
    "        bootstrap += 1\n",
    "\n",
    "    time_5_bootstraps = time.time() - start_time\n",
    "    \n",
    "    n_bootstraps = bootstrap - 1\n",
    "    \n",
    "    print('### Number of bootstraps is', n_bootstraps)\n",
    "    \n",
    "    bootstrap_auc_mean = np.mean(bootstrap_aucs)\n",
    "    bootstrap_auc_std = np.std(bootstrap_aucs)\n",
    "    \n",
    "    print('### Mean AUC for grid search', gridsearch, ':', round(bootstrap_auc_mean,2),'±', round(bootstrap_auc_std,3))\n",
    "    \n",
    "    bootstrap_epoch_mean = np.mean(bootstrap_epoch)\n",
    "    bootstrap_epoch_std = np.std(bootstrap_epoch)\n",
    "    \n",
    "    print('### Mean n epochs for grid search', gridsearch, ':', round(bootstrap_epoch_mean,2),'±', round(bootstrap_epoch_std,3))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    bootstrap_time_column.append(time_5_bootstraps)\n",
    "    double_layer_column.append(double_layer)\n",
    "    dropout_column.append(dropout)\n",
    "    embedding_dim_column.append(embedding_dim)\n",
    "    filter_size_column.append(filter_size)\n",
    "    l1_column.append(l1)\n",
    "    l2_column.append(l2)\n",
    "    learning_rate_column.append(learning_rate)\n",
    "    max_len_column.append(max_len)\n",
    "    max_pooling_column.append(max_pooling)\n",
    "    max_words_column.append(max_words)\n",
    "    n_filters_dense_column.append(n_filters_dense)\n",
    "    n_filters_conv_column.append(n_filters_conv)\n",
    "    n_layers_conv_column.append(n_layers_conv)\n",
    "    n_layers_dense_column.append(n_layers_dense)\n",
    "    n_params_column.append(model.count_params())\n",
    "    optimizer_type_column.append(optimizer_type)\n",
    "    sample_size_column.append(sample_size)\n",
    "    auc_mean.append(bootstrap_auc_mean)\n",
    "    auc_std.append(bootstrap_auc_std)\n",
    "    epochmeans.append(bootstrap_epoch_mean)\n",
    "    epochstds.append(bootstrap_epoch_std)\n",
    "    \n",
    "    gridsearch += 1\n",
    "\n",
    "    \n",
    "df_performance = pd.DataFrame(list(zip(sample_size_column, embedding_dim_column, max_len_column, max_words_column, \n",
    "                                       n_layers_conv_column, double_layer_column, n_filters_conv_column, filter_size_column,\n",
    "                                       max_pooling_column, n_layers_dense_column, n_filters_dense_column,\n",
    "                                       l1_column, l2_column, dropout_column, n_params_column,\n",
    "                                       auc_mean, auc_std, epochmeans, epochstds, optimizer_type_column, learning_rate_column, bootstrap_time_column)))\n",
    "\n",
    "\n",
    "df_performance.columns = ['sample_size', 'embedding_dim', 'max_len', 'max_words',\n",
    "                         'n_layers_conv', 'double_layer', 'n_filters_conv', 'filter_size',\n",
    "                         'max_pooling', 'n_layers_dense', 'n_filters_dense',\n",
    "                         'l1_column', 'l2_column', 'dropout', 'n_params',\n",
    "                         'auc_mean', 'auc_std', 'epochmeans', 'epochstds',\n",
    "                         'optimizer_type_column', 'learning_rate_column', 'bootstrap_time_column']\n",
    "\n",
    "target_dir = ### INSERT TARGET DIRECTORY ###\n",
    "\n",
    "df_performance.to_excel(os.path.join(target_dir, 'hyperparam_tuning_cnn.xlsx'))\n",
    "\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4D. Create optimal parametergrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Because the bootstrapped predictions on the final test required significant computational time,\n",
    "### the results were computed in 4 batches based on the sample size and merged together.\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid_dict = { \n",
    "    'double_layer': [0], #<- final\n",
    "    'dropout': [0.5], #<- final\n",
    "    'embedding_dim': [128],# <- final\n",
    "    'filter_size': [3], #<- final \n",
    "    'l1': [0],#<- final\n",
    "    'l2': [0], #<- final\n",
    "    'learning_rate': [0.01], #<- final\n",
    "    'max_len': [1000],#<- final\n",
    "    'max_pooling' : [1],#<- final\n",
    "    'max_words': [1000],#<- final\n",
    "    'n_filters_conv:': [0],#<- final\n",
    "    'n_filters_dense':[0],#<- final\n",
    "    'n_layers_conv': [0],#<- final\n",
    "    'n_layers_dense':[0],#<- final\n",
    "    'optimizer_type': ['adam'],\n",
    "    'sample_size': [25, 50, 100, 150, 200, 250, 300, 400, 500, 750, 1000, 1500, 2000, 3000],\n",
    "                  }\n",
    "\n",
    "param_grid_list = list(ParameterGrid(param_grid_dict))\n",
    "\n",
    "parameter_grid = []\n",
    "for dict in param_grid_list:\n",
    "    hyperparameters = list(dict.values())\n",
    "    parameter_grid.append(hyperparameters)\n",
    "\n",
    "\n",
    "print('Length parameter grid is', len(parameter_grid), 'hyperparameter settings')\n",
    "\n",
    "parameter_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4E. Create final performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both the CNN and non-CNN model architecture can be evaluated by specifying the get_conv1d_model2 function\n",
    "\n",
    "from keras.layers import Embedding, Flatten, Dense, Input, LSTM, Conv1D, Dropout, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras import preprocessing\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "bootstrap_time_column = []\n",
    "n_layers_dense_column = []\n",
    "double_layer_column = []\n",
    "dropout_column = []\n",
    "embedding_dim_column = []\n",
    "filter_size_column = []\n",
    "l1_column = []\n",
    "l2_column = []\n",
    "learning_rate_column = []\n",
    "max_len_column = []\n",
    "max_pooling_column = []\n",
    "max_words_column = []\n",
    "n_filters_dense_column = []\n",
    "n_filters_conv_column = []\n",
    "n_layers_conv_column = []\n",
    "n_params_column = []\n",
    "optimizer_type_column = []\n",
    "sample_size_column = []\n",
    "auc_mean = []\n",
    "auc_std = []\n",
    "epochmeans = []\n",
    "epochstds = []\n",
    "\n",
    "gridsearch = 1\n",
    "\n",
    "def get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(dropout))\n",
    "    if n_layers_conv == 1:\n",
    "        model.add(layers.Conv1D(n_filters_conv, filter_size, activation='relu', kernel_regularizer=regularizers.l1_l2(l1, l2)))\n",
    "    if max_pooling == 1:\n",
    "        model.add(layers.MaxPooling1D(filter_size))\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    if n_layers_dense == 1:\n",
    "        model.add(Dense(n_filters_dense, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=final_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def square(list):\n",
    "    return [i ** 2 for i in list]\n",
    "\n",
    "for double_layer, dropout, embedding_dim, filter_size, l1, l2, learning_rate, max_len, max_pooling, max_words, n_filters_conv, n_filters_dense, n_layers_conv, n_layers_dense, optimizer_type, sample_size in parameter_grid:\n",
    "    \n",
    "    print('##### Grid Search', gridsearch,'/', len(parameter_grid))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(df['stemmed_report'])\n",
    "    sequences = tokenizer.texts_to_sequences(df['stemmed_report'])\n",
    "    \n",
    "    df['vectorized_report'] = sequences \n",
    "    \n",
    "    bootstrap_aucs = []\n",
    "    bootstrap_auc_stds = []\n",
    "    \n",
    "    bootstrap_epoch = []\n",
    "    \n",
    "    bootstrap = 1\n",
    "\n",
    "    if optimizer_type == 'adam':\n",
    "        final_optimizer = optimizers.Adam(lr=learning_rate)\n",
    "    elif optimizer_type == 'nadam':\n",
    "        final_optimizer = optimizers.Nadam(lr=learning_rate)\n",
    "    \n",
    "    model = get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer)\n",
    "    model.summary()\n",
    "\n",
    "    start_time = time.time()\n",
    "          \n",
    "    for n in range(100):\n",
    "        \n",
    "        print('--- Bootstrap', bootstrap)\n",
    "\n",
    "        df_frac = df[(df['cohort'] == 'train')].sample(n = 2*sample_size, replace = True)\n",
    "\n",
    "        array_train = array(list(df_frac['vectorized_report'][(df_frac['cohort'] == 'train')]))\n",
    "        array_test = array(list(df['vectorized_report'][(df['cohort'] == 'test')]))\n",
    "\n",
    "        X_train = preprocessing.sequence.pad_sequences(array_train, maxlen=max_len)\n",
    "        X_test = preprocessing.sequence.pad_sequences(array_test, maxlen=max_len)\n",
    "        \n",
    "        y_train = array(df_frac[['Glioma', 'Meningioma', 'Metastasis']][(df_frac['cohort'] == 'train')])\n",
    "        y_test = array(df[['Glioma', 'Meningioma', 'Metastasis']][(df['cohort'] == 'test')])\n",
    "        \n",
    "        model = get_conv1d_model2(n_layers_conv, n_filters_conv, filter_size, l1, l2, max_pooling, n_layers_dense, n_filters_dense, dropout, final_optimizer)\n",
    "        \n",
    "        try:\n",
    "            history = model.fit(X_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                verbose = 0)\n",
    "            \n",
    "            auc = roc_auc_score(y_test, model.predict_proba(X_test), average='weighted')\n",
    "            n_epochs = len(history.epoch)\n",
    "        except ValueError:\n",
    "                pass\n",
    "        \n",
    "        print('AUC:', str(round(auc,3)))\n",
    "        bootstrap_aucs.append(auc)\n",
    "\n",
    "        print('n epochs:', str(round(n_epochs,3)))\n",
    "        bootstrap_epoch.append(n_epochs)\n",
    "        \n",
    "        bootstrap += 1\n",
    "\n",
    "    time_100_bootstraps = time.time() - start_time\n",
    "    \n",
    "    n_bootstraps = bootstrap - 1\n",
    "    print('### Number of bootstraps is', n_bootstraps)\n",
    "    \n",
    "    bootstrap_auc_mean = np.mean(bootstrap_aucs)\n",
    "    bootstrap_auc_std = np.std(bootstrap_aucs)\n",
    "    print('### Mean AUC for grid search', gridsearch, ':', round(bootstrap_auc_mean,2),'±', round(bootstrap_auc_std,3))\n",
    "\n",
    "    bootstrap_epoch_mean = np.mean(bootstrap_epoch)\n",
    "    bootstrap_epoch_std = np.std(bootstrap_epoch)\n",
    "    print('### Mean n epochs for grid search', gridsearch, ':', round(bootstrap_epoch_mean,2),'±', round(bootstrap_epoch_std,3))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    bootstrap_time_column.append(time_100_bootstraps)\n",
    "    double_layer_column.append(double_layer)\n",
    "    dropout_column.append(dropout)\n",
    "    embedding_dim_column.append(embedding_dim)\n",
    "    filter_size_column.append(filter_size)\n",
    "    l1_column.append(l1)\n",
    "    l2_column.append(l2)\n",
    "    learning_rate_column.append(learning_rate)\n",
    "    max_len_column.append(max_len)\n",
    "    max_pooling_column.append(max_pooling)\n",
    "    max_words_column.append(max_words)\n",
    "    n_filters_dense_column.append(n_filters_dense)\n",
    "    n_filters_conv_column.append(n_filters_conv)\n",
    "    n_layers_conv_column.append(n_layers_conv)\n",
    "    n_layers_dense_column.append(n_layers_dense)\n",
    "    n_params_column.append(model.count_params())\n",
    "    optimizer_type_column.append(optimizer_type)\n",
    "    sample_size_column.append(sample_size)\n",
    "    auc_mean.append(bootstrap_auc_mean)\n",
    "    auc_std.append(bootstrap_auc_std)\n",
    "    epochmeans.append(bootstrap_epoch_mean)\n",
    "    epochstds.append(bootstrap_epoch_std)\n",
    "    \n",
    "    gridsearch += 1\n",
    "\n",
    "    \n",
    "df_performance = pd.DataFrame(list(zip(sample_size_column, embedding_dim_column, max_len_column, max_words_column, \n",
    "                                       n_layers_conv_column, double_layer_column, n_filters_conv_column, filter_size_column,\n",
    "                                       max_pooling_column, n_layers_dense_column, n_filters_dense_column,\n",
    "                                       l1_column, l2_column, dropout_column, n_params_column,\n",
    "                                       auc_mean, auc_std,\n",
    "                                       epochmeans, epochstds, optimizer_type_column, learning_rate_column, bootstrap_time_column)))\n",
    "\n",
    "\n",
    "df_performance.columns = ['sample_size', 'embedding_dim', 'max_len', 'max_words',\n",
    "                         'n_layers_conv', 'double_layer', 'n_filters_conv', 'filter_size',\n",
    "                         'max_pooling', 'n_layers_dense', 'n_filters_dense',\n",
    "                         'l1_column', 'l2_column', 'dropout', 'n_params',\n",
    "                         'auc_mean', 'auc_std', 'epochmeans', 'epochstds',\n",
    "                         'optimizer_type_column', 'learning_rate_column', 'bootstrap_time_column']\n",
    "\n",
    "target_dir = ### INSERT TARGET DIRECTORY ###\n",
    "\n",
    "df_performance.to_excel(os.path.join(target_dir, 'hyperparam_tuning_cnn.xlsx'))\n",
    "\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4F. Merge final results of the classic CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.751109</td>\n",
       "      <td>0.084934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.835083</td>\n",
       "      <td>0.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>0.885162</td>\n",
       "      <td>0.051688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.903843</td>\n",
       "      <td>0.037899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.924591</td>\n",
       "      <td>0.027612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.935970</td>\n",
       "      <td>0.024382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>0.942671</td>\n",
       "      <td>0.023917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>0.944810</td>\n",
       "      <td>0.019735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400</td>\n",
       "      <td>0.953649</td>\n",
       "      <td>0.015151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>0.956211</td>\n",
       "      <td>0.014626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>600</td>\n",
       "      <td>0.956417</td>\n",
       "      <td>0.013727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>800</td>\n",
       "      <td>0.961979</td>\n",
       "      <td>0.011663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.963028</td>\n",
       "      <td>0.013137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.963276</td>\n",
       "      <td>0.013141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.965119</td>\n",
       "      <td>0.012345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.964669</td>\n",
       "      <td>0.013324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2100</td>\n",
       "      <td>0.969236</td>\n",
       "      <td>0.011023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.964373</td>\n",
       "      <td>0.012998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.966035</td>\n",
       "      <td>0.013104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_size  auc_mean   auc_std\n",
       "0            25  0.751109  0.084934\n",
       "1            50  0.835083  0.062300\n",
       "2            75  0.885162  0.051688\n",
       "3           100  0.903843  0.037899\n",
       "4           150  0.924591  0.027612\n",
       "5           200  0.935970  0.024382\n",
       "6           250  0.942671  0.023917\n",
       "7           300  0.944810  0.019735\n",
       "8           400  0.953649  0.015151\n",
       "9           500  0.956211  0.014626\n",
       "10          600  0.956417  0.013727\n",
       "11          800  0.961979  0.011663\n",
       "12         1000  0.963028  0.013137\n",
       "13         1200  0.963276  0.013141\n",
       "14         1500  0.965119  0.012345\n",
       "15         1800  0.964669  0.013324\n",
       "16         2100  0.969236  0.011023\n",
       "17         2500  0.964373  0.012998\n",
       "18         3000  0.966035  0.013104"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Because the bootstrapped predictions on the final test required significant computational time,\n",
    "### the results were computed in 4 batches based on the sample size and merged together.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path_final_results = ### INSERT TARGET DIRECTORY ###\n",
    "df_results_part_1 = pd.read_excel(os.path.join(path_final_results, 'part_1.xlsx'))\n",
    "df_results_part_2 = pd.read_excel(os.path.join(path_final_results, 'part_2.xlsx'))\n",
    "df_results_part_3 = pd.read_excel(os.path.join(path_final_results, 'part_3.xlsx'))\n",
    "df_results_part_4 = pd.read_excel(os.path.join(path_final_results, 'part_4.xlsx'))\n",
    "\n",
    "df_results_cnn = df_results_part_1.append(df_results_part_2, ignore_index=True)\n",
    "df_results_cnn = df_results_cnn.append(df_results_part_3, ignore_index=True)\n",
    "df_results_cnn = df_results_cnn.append(df_results_part_4, ignore_index=True)\n",
    "\n",
    "df_results_cnn = df_results_cnn[['sample_size', 'auc_mean', 'auc_std']]\n",
    "\n",
    "df_results_cnn.to_excel(os.path.join(path_final_results, 'final_results.xlsx'))\n",
    "df_results_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4G. Merge final results of the ClinicalTextMiner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>0.793640</td>\n",
       "      <td>0.052155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.900822</td>\n",
       "      <td>0.038104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75</td>\n",
       "      <td>0.943463</td>\n",
       "      <td>0.022527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>0.957711</td>\n",
       "      <td>0.016466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150</td>\n",
       "      <td>0.976549</td>\n",
       "      <td>0.008033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.982854</td>\n",
       "      <td>0.004884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>250</td>\n",
       "      <td>0.985870</td>\n",
       "      <td>0.002757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>0.986877</td>\n",
       "      <td>0.001901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>400</td>\n",
       "      <td>0.988390</td>\n",
       "      <td>0.001665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>500</td>\n",
       "      <td>0.989212</td>\n",
       "      <td>0.001516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>600</td>\n",
       "      <td>0.989729</td>\n",
       "      <td>0.001112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>800</td>\n",
       "      <td>0.990664</td>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.990998</td>\n",
       "      <td>0.000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.991282</td>\n",
       "      <td>0.000858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.991514</td>\n",
       "      <td>0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.991811</td>\n",
       "      <td>0.000826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2100</td>\n",
       "      <td>0.991744</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2500</td>\n",
       "      <td>0.992130</td>\n",
       "      <td>0.000712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.992270</td>\n",
       "      <td>0.000656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_size  auc_mean   auc_std\n",
       "0            25  0.793640  0.052155\n",
       "1            50  0.900822  0.038104\n",
       "2            75  0.943463  0.022527\n",
       "3           100  0.957711  0.016466\n",
       "4           150  0.976549  0.008033\n",
       "5           200  0.982854  0.004884\n",
       "6           250  0.985870  0.002757\n",
       "7           300  0.986877  0.001901\n",
       "8           400  0.988390  0.001665\n",
       "9           500  0.989212  0.001516\n",
       "10          600  0.989729  0.001112\n",
       "11          800  0.990664  0.000904\n",
       "12         1000  0.990998  0.000902\n",
       "13         1200  0.991282  0.000858\n",
       "14         1500  0.991514  0.000875\n",
       "15         1800  0.991811  0.000826\n",
       "16         2100  0.991744  0.000763\n",
       "17         2500  0.992130  0.000712\n",
       "18         3000  0.992270  0.000656"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Because the bootstrapped predictions on the final test required significant computational time,\n",
    "### the results were computed in 4 batches based on the sample size and subsequently merged together.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path_final_results = ### INSERT TARGET DIRECTORY ###\n",
    "\n",
    "df_results_embed = pd.read_excel(os.path.join(path_final_results, 'part_1.xlsx'))\n",
    "df_results_part_1 = pd.read_excel(os.path.join(path_final_results, 'part_1.xlsx'))\n",
    "df_results_part_2 = pd.read_excel(os.path.join(path_final_results, 'part_2.xlsx'))\n",
    "df_results_part_3 = pd.read_excel(os.path.join(path_final_results, 'part_3.xlsx'))\n",
    "df_results_part_4 = pd.read_excel(os.path.join(path_final_results, 'part_4.xlsx'))\n",
    "\n",
    "df_results_embed = df_results_part_1.append(df_results_part_2, ignore_index=True)\n",
    "df_results_embed = df_results_embed.append(df_results_part_3, ignore_index=True)\n",
    "df_results_embed = df_results_embed.append(df_results_part_4, ignore_index=True)\n",
    "\n",
    "df_results_embed = df_results_embed[['sample_size', 'auc_mean', 'auc_std']]\n",
    "\n",
    "df_results_embed.to_excel(os.path.join(path_final_results, 'final_results.xlsx'))\n",
    "df_results_embed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
